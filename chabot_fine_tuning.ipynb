{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BazanNicolas/Chabot-Fine-Tuning/blob/main/chabot_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qghu8nMe45Vh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "2qB3a-IzAlcR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del chat de WhatsApp para la creación del dataset\n",
        "\n",
        "Primero debemos cargar un archivo de chat de WhatsApp (llamado chat.txt por defecto). Luego filtramos mensajes irrelevantes, como por ejemplo aquellos donde se encontraban mensajes multimedia, y separamos las conversaciones en prompts y responses. Además capturamos los mensajes del autor objetivo (\"Nico Bazan\" en este caso) y los emparejamos con los mensajes de los otros participantes del chat sin importar sus nombres para usarlos como entrenamiento.\n",
        "Es importante notar que también se verifica que los mensajes no estén vacíos."
      ],
      "metadata": {
        "id": "4TFfuTBzCfrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos irrelevantes a ignorar\n",
        "irrelevantData = {\n",
        "    'Eliminaste este mensaje.',\n",
        "    'Se eliminó este mensaje.',\n",
        "    '<Multimedia omitido>',\n",
        "    'You deleted this message',\n",
        "    'This message was deleted',\n",
        "    '<Media omitted>'\n",
        "}\n",
        "\n",
        "# Verificar si el mensaje es irrelevante\n",
        "def containsIrrelevantData(message):\n",
        "    return any(irrelevant in message for irrelevant in irrelevantData)\n",
        "\n",
        "# Procesar el chat con múltiples autores\n",
        "def process_whatsapp_chat(filepath, target_author=\"Nico Bazan\"):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        chat_lines = file.readlines()\n",
        "\n",
        "    prompts = []\n",
        "    responses = []\n",
        "\n",
        "    current_prompt = \"\"\n",
        "    current_response = \"\"\n",
        "    in_prompt = True  # Indica si estamos acumulando en el prompt\n",
        "\n",
        "    for line in chat_lines:\n",
        "        # Extraer fecha, autor y contenido del mensaje\n",
        "        match = re.match(r'\\d+/\\d+/\\d+,\\s\\d+:\\d+\\s-\\s([^:]+):\\s(.+)', line)\n",
        "\n",
        "        if match:\n",
        "            author = match.group(1).strip()\n",
        "            message = match.group(2).strip()\n",
        "\n",
        "            # Ignorar mensajes irrelevantes\n",
        "            if containsIrrelevantData(message):\n",
        "                continue\n",
        "\n",
        "            if author != target_author:  # Otros autores\n",
        "                if in_prompt:\n",
        "                    current_prompt += f\" {message}\" if current_prompt else message\n",
        "                else:\n",
        "                    # Guardar el par si no está vacío\n",
        "                    if current_prompt.strip() and current_response.strip():\n",
        "                        prompts.append(current_prompt.strip())\n",
        "                        responses.append(current_response.strip())\n",
        "                    # Reiniciar para el siguiente ciclo\n",
        "                    current_prompt = message\n",
        "                    current_response = \"\"\n",
        "                    in_prompt = True  # Volver a modo prompt\n",
        "            else:  # Mensajes del autor objetivo (Nico Bazan)\n",
        "                if not in_prompt:\n",
        "                    current_response += f\" {message}\" if current_response else message\n",
        "                else:\n",
        "                    in_prompt = False  # Cambiar a modo respuesta\n",
        "                    current_response = message\n",
        "\n",
        "    # Guardar el último par si no está vacío\n",
        "    if current_prompt.strip() and current_response.strip():\n",
        "        prompts.append(current_prompt.strip())\n",
        "        responses.append(current_response.strip())\n",
        "\n",
        "    return prompts, responses\n",
        "\n",
        "# Procesar el chat\n",
        "chat_file = 'chat.txt'\n",
        "target_author = 'Nico Bazan'\n",
        "prompts, responses = process_whatsapp_chat(chat_file, target_author)\n",
        "\n",
        "# Eliminar pares vacíos\n",
        "prompts, responses = zip(*[\n",
        "    (p, r) for p, r in zip(prompts, responses) if p.strip() and r.strip()\n",
        "])\n",
        "\n",
        "# Verificar los resultados\n",
        "print(\"Ejemplo de Prompts:\", prompts[:2])\n",
        "print(\"Ejemplo de Responses:\", responses[:2])\n"
      ],
      "metadata": {
        "id": "JaaCwqA85JKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir 3 ejemplos limpios de prompts y responses\n",
        "for i in range(min(3, len(prompts))):\n",
        "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
        "    print(f\"Response {i+1}: {responses[i]}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "FQT-xZ1K-5jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del archivo de entrenamiento con delimitadores\n",
        "\n",
        "Convertiremos los prompts y responses procesados en un formato estructurado utilizando delimitadores claros (PROMPT, RESPONSE y ###). Cada par de conversación se guarda en el archivo train_data.txt para su posterior uso en el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "x3tRuvFuD2U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear archivo con delimitadores claros para prompt y response\n",
        "train_data = \"\"\n",
        "for prompt, response in zip(prompts, responses):\n",
        "    train_data += f\"PROMPT:\\n{prompt}\\nRESPONSE:\\n{response}\\n###\\n\"  # Delimitadores claros\n",
        "\n",
        "# Guardar los datos en 'train_data.txt'\n",
        "with open('train_data.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(train_data)\n",
        "\n",
        "print(\"Datos guardados correctamente en 'train_data.txt'.\")\n"
      ],
      "metadata": {
        "id": "HAI5oVsR_AGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y configuración del modelo GPT-2 en español\n",
        "\n",
        "Ahora realizamos la configuración del modelo GPT-2 en español que será fine tuneado utilizando la librería transformers. Además, se ajustan parámetros como:\n",
        "\n",
        "* Carga del modelo base y el tokenizador GPT-2 con un model_max_length de 64.\n",
        "* Configuración del dropout (comentada inicialmente, pero se puede ajustar para el entrenamiento).\n",
        "* Alineación del pad_token con `eos_token,** garantizando que el padding no interfiera con el procesamiento del modelo.\n",
        "* Ajuste de embeddings: Se asegura de que la dimensión del vocabulario se adapte al tokenizador cargado.\n",
        "* Sincronización del peso de la capa de salida (lm_head) con los embeddings iniciales para evitar inconsistencias."
      ],
      "metadata": {
        "id": "stPFUorYcD2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xYrPX5s0QFEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cargar el modelo base en español\n",
        "model_name = \"DeepESP/gpt2-spanish\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=64)\n",
        "\n",
        "# Configurar dropout (esto se aplicará durante el entrenamiento)\n",
        "# model.config.attn_pdrop = 0.3\n",
        "# model.config.embd_pdrop = 0.3\n",
        "# model.config.resid_pdrop = 0.3\n",
        "\n",
        "\n",
        "# Alinear el token de padding con el token de fin de secuencia\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))  # Ajustar embeddings\n",
        "\n",
        "# Inicializa el peso de la lm_head si es necesario\n",
        "with torch.no_grad():\n",
        "    model.lm_head.weight = model.transformer.wte.weight\n"
      ],
      "metadata": {
        "id": "U5XXYZhjcDx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y creación del dataset para entrenamiento y validación\n",
        "\n",
        "Esta celda carga los datos desde el archivo train_data.txt, los procesa utilizando delimitadores claros y los convierte en un dataset compatible con Hugging Face. A continuación, el dataset se divide en subconjuntos para entrenamiento y validación (80-20), garantizando que los datos se mezclen aleatoriamente."
      ],
      "metadata": {
        "id": "sbq8YqfWaPU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Función para leer el archivo y agrupar los ejemplos con delimitadores claros\n",
        "def load_grouped_messages(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    # Usar regex para extraer prompts y responses con delimitadores claros\n",
        "    examples = re.findall(r\"PROMPT:\\n(.*?)\\nRESPONSE:\\n(.*?)\\n###\", raw_data, re.DOTALL)\n",
        "\n",
        "    if not examples:\n",
        "        raise ValueError(\"No se encontraron ejemplos en el archivo. Verifica el formato.\")\n",
        "\n",
        "    # Crear lista de diccionarios con prompts y responses separados\n",
        "    data = [{\"prompt\": prompt.strip(), \"response\": response.strip()} for prompt, response in examples]\n",
        "    return data\n",
        "\n",
        "# Cargar los datos desde el archivo\n",
        "data = load_grouped_messages(\"train_data.txt\")\n",
        "\n",
        "# Crear un Dataset de Huggingface a partir de los datos cargados\n",
        "dataset = Dataset.from_dict({\n",
        "    \"prompt\": [d[\"prompt\"] for d in data],\n",
        "    \"response\": [d[\"response\"] for d in data]\n",
        "})\n",
        "\n",
        "# Dividir el dataset en entrenamiento y validación, asegurando mezcla\n",
        "train_test_split = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
        "\n",
        "# Crear un DatasetDict para entrenamiento y validación\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_test_split[\"train\"],\n",
        "    \"validation\": train_test_split[\"test\"]\n",
        "})\n",
        "\n",
        "# Verificar la estructura del dataset\n",
        "print(\"Primeros ejemplos de entrenamiento:\\n\", datasets[\"train\"][:2])\n"
      ],
      "metadata": {
        "id": "CynPKzyBOBpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de funciones auxiliares para tokenización y agrupación de textos\n",
        "\n",
        "Esta celda define dos funciones auxiliares esenciales para la preparación del dataset y el entrenamiento del modelo. Estas funciones se encargan de tokenizar los textos y agruparlos en bloques del tamaño adecuado para optimizar el entrenamiento del modelo.\n",
        "Devuelven un nuevo dataset en el que los textos están tokenizados (convertidos en índices del vocabulario) y se incluye una máscara de atención para distinguir los tokens relevantes de los tokens de padding. Además, se preparan labels para el entrenamiento.\n",
        "\n",
        "(Créditos a [Cristian Cardellino](https://crscardellino.net) por estas [funciones](https://huggingface.co/crscardellino/flisol-cba-martin-fierro/resolve/main/utils.py))"
      ],
      "metadata": {
        "id": "xH7h4NSyFQFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "from typing import Callable, Dict, List\n",
        "\n",
        "\n",
        "def tokenize(\n",
        "    tokenizer: PreTrainedTokenizerBase, end_char: str = \"\\n\"\n",
        ") -> Callable[[Dict[str, List[str]]], DatasetDict]:\n",
        "    \"\"\"\n",
        "    Tokeniza los textos agregando un caracter final opcional (`end_char`).\n",
        "    \"\"\"\n",
        "\n",
        "    def _tokenize(examples: Dict[str, List[str]]) -> DatasetDict:\n",
        "        # Asegurarse de que cada entrada tiene texto válido\n",
        "        texts = [f\"{e}{end_char}\" for e in examples[\"text\"] if e.strip()]\n",
        "        return tokenizer(texts, truncation=True, padding=True)\n",
        "\n",
        "    return _tokenize\n",
        "\n",
        "\n",
        "def group_texts(examples: Dict[str, List[int]], block_size: int = 128) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Agrupa los textos en bloques del tamaño especificado y genera etiquetas (labels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenar todos los tokens\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[\"input_ids\"])\n",
        "\n",
        "    if total_length < block_size:\n",
        "        raise ValueError(\"Los datos son insuficientes para formar un bloque del tamaño especificado.\")\n",
        "\n",
        "    # Ajustar el total_length para evitar remainders incompletos\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # Dividir en bloques del tamaño especificado\n",
        "    result = {\n",
        "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "    # Copiar input_ids como labels para el entrenamiento\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "oqCeFuouBzLu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización del dataset para preparación del entrenamiento\n",
        "\n",
        "En esta celda se realiza la tokenización de los datos de entrenamiento y validación, preparando los textos para que puedan ser procesados por el modelo. La función de tokenización transforma los textos en índices del vocabulario y añade padding para mantener la consistencia en la longitud de los ejemplos."
      ],
      "metadata": {
        "id": "1fpw9SB0ahhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "'''\n",
        "Propósito: Combina los prompts y responses en un solo string, los tokeniza y asegura que todos los ejemplos tengan la misma longitud (128 tokens).\n",
        "Truncamiento: Corta los textos si superan el límite definido (128 tokens).\n",
        "Padding: Añade tokens de relleno hasta alcanzar 128 tokens para garantizar que todos los ejemplos tengan la misma longitud.\n",
        "Retorno: Devuelve los tensores en formato PyTorch (return_tensors=\"pt\").\n",
        "'''\n",
        "def tokenize_function(examples):\n",
        "    # Concatenar prompt y response en un solo string\n",
        "    texts = [f\"{p} {r}\".strip() for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,  # Truncar si excede el límite\n",
        "        max_length=128,    # Limitar a 128 tokens\n",
        "        padding=\"max_length\",  # Rellenar hasta 128 tokens\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Usar partial para fijar el tokenizador en la función\n",
        "tokenize_with_tokenizer = partial(tokenize_function)\n",
        "\n",
        "# Aplicar la tokenización al dataset usando el partial\n",
        "tokenized_datasets = datasets.map(\n",
        "    tokenize_with_tokenizer,  # Usar la función parcial\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"prompt\", \"response\"]  # Remover columnas originales\n",
        ")\n",
        "\n",
        "# Verificar los primeros datos tokenizados\n",
        "print(\"Datos tokenizados:\\n\", tokenized_datasets[\"train\"][:2])\n"
      ],
      "metadata": {
        "id": "hBzd_JUSahbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agrupación de textos en bloques del tamaño especificado\n",
        "\n",
        "Se aplica la función group_texts al dataset previamente tokenizado para dividir los tokens en bloques del tamaño especificado (por defecto, 128 tokens). Esto asegura que los lotes (batches) tengan una longitud uniforme durante el entrenamiento."
      ],
      "metadata": {
        "id": "2K0JJOHbG2AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar los textos en bloques del tamaño especificado\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=1  # Cambiar a 1 si estás usando Google Colab sin núcleos adicionales\n",
        ")\n",
        "\n",
        "# Verificar los primeros ejemplos agrupados (mostrar solo los primeros 20 tokens)\n",
        "print(\"Datos agrupados (primeros 20 tokens):\\n\",\n",
        "      {k: v[:20] for k, v in lm_datasets[\"train\"][:2].items()})\n"
      ],
      "metadata": {
        "id": "U58Fx_77CPn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entrenamiento y optimización del modelo\n",
        "\n",
        "Se configura los parámetros clave para el entrenamiento del modelo mediante el uso de TrainingArguments y el objeto Trainer. Se define el optimizador AdamW con un scheduler cosine para ajustar dinámicamente la tasa de aprendizaje. Además, se calcula el número de pasos totales y de calentamiento (warmup) para controlar la tasa al inicio del entrenamiento. La configuración incluye la opción de EarlyStopping para detener el entrenamiento si no hay mejora en la pérdida, optimizando así los recursos."
      ],
      "metadata": {
        "id": "6tLNtlNlavWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
        "\n",
        "# Argumentos del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_gpt2\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluar después de cada época\n",
        "    save_strategy=\"epoch\",  # Guardar el modelo al final de cada época\n",
        "    num_train_epochs=3,  # Número de épocas\n",
        "    per_device_train_batch_size=8,  # Tamaño del batch\n",
        "    learning_rate=1e-5,  # Tasa de aprendizaje\n",
        "    weight_decay=0.01,  # Regularización\n",
        "    logging_steps=5,  # Log cada 5 pasos\n",
        "    report_to=[],  # Deshabilitar W&B\n",
        "    load_best_model_at_end=True,  # Cargar el mejor modelo al final\n",
        "    metric_for_best_model=\"loss\",  # Usar 'loss' como métrica principal\n",
        "    greater_is_better=False  # Minimizar la pérdida\n",
        ")\n",
        "\n",
        "# Definir el optimizador usando los argumentos\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "# Calcular el número total de pasos de entrenamiento\n",
        "num_training_steps = (\n",
        "    len(lm_datasets[\"train\"]) // training_args.per_device_train_batch_size\n",
        ") * training_args.num_train_epochs\n",
        "\n",
        "num_warmup_steps = int(num_training_steps * 0.1)  # Warmup del 10%\n",
        "scheduler = get_scheduler(\n",
        "    \"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(num_training_steps * 0.1),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "\n",
        "# Configurar el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        "    optimizers=(optimizer, scheduler),  # Optimizer y scheduler\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Detener si no mejora\n",
        ")\n"
      ],
      "metadata": {
        "id": "u1F_02_vavR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio del Entrenamiento del Modelo\n",
        "Ejecuta el proceso de fine-tuning"
      ],
      "metadata": {
        "id": "oz4XNjNoa1D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sMXpnuNsa1AP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8fdb66e6-2661-4069-f4f2-a3931176d2a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6699' max='6699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6699/6699 38:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.161100</td>\n",
              "      <td>1.051037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.189500</td>\n",
              "      <td>1.019949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.912100</td>\n",
              "      <td>1.016659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6699, training_loss=1.0928075167292497, metrics={'train_runtime': 2334.4003, 'train_samples_per_second': 22.952, 'train_steps_per_second': 2.87, 'total_flos': 3500006768640000.0, 'train_loss': 1.0928075167292497, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del Modelo: Perplejidad y Entropía Cruzada (No ejecutar por ahora, Cuda se queda sin memoria 😭)\n",
        "\n",
        "Esta celda evalúa el rendimiento del modelo mediante dos métricas: perplejidad y entropía cruzada. La perplejidad se calcula a partir de la pérdida en el conjunto de validación y refleja qué tan bien el modelo predice secuencias de texto. La entropía cruzada mide la discrepancia entre las distribuciones de los logits del modelo(predicciones sin procesar) y las etiquetas reales.\n",
        "En otras palabras, mide cuán diferentes son estas dos distribuciones: la predicción del modelo (logits) y la distribución \"objetivo\" (la palabra correcta con probabilidad 1). Una"
      ],
      "metadata": {
        "id": "Z8ZMltKSHw07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def compute_perplexity(eval_loss):\n",
        "    \"\"\"Calcula la perplejidad a partir de la pérdida.\"\"\"\n",
        "    return math.exp(eval_loss)\n",
        "\n",
        "def compute_cross_entropy(logits, labels):\n",
        "    \"\"\"Calcula la entropía cruzada entre los logits y las etiquetas reales.\"\"\"\n",
        "    shift_logits = logits[..., :-1, :].contiguous()  # Desplazar logits\n",
        "    shift_labels = labels[..., 1:].contiguous()  # Desplazar etiquetas\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1)\n",
        "    )\n",
        "    return loss.item()\n",
        "\n",
        "# Liberar memoria ANTES de la evaluación\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Evaluar el modelo con batches pequeños para evitar problemas de memoria\n",
        "valid_dataset = lm_datasets[\"validation\"]\n",
        "batch_size = 2  # Ajustar según la memoria disponible\n",
        "\n",
        "logits_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(valid_dataset), batch_size):\n",
        "        batch = valid_dataset[i: i + batch_size]\n",
        "\n",
        "        # Preparar inputs y moverlos a GPU\n",
        "        inputs = {\n",
        "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
        "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device),\n",
        "        }\n",
        "        labels = torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "        # Generar logits y moverlos inmediatamente a la CPU para liberar memoria GPU\n",
        "        outputs = model(**inputs)\n",
        "        logits_list.append(outputs.logits.cpu())\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "        # Liberar memoria GPU después de cada lote\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# Concatenar todos los logits y etiquetas\n",
        "logits = torch.cat(logits_list, dim=0)\n",
        "all_labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "# Calcular la entropía cruzada con los logits concatenados\n",
        "cross_entropy = compute_cross_entropy(logits, all_labels)\n",
        "print(f\"Entropía Cruzada: {cross_entropy}\")\n",
        "\n",
        "# Evaluar el modelo usando el Trainer y mostrar la perplejidad\n",
        "eval_result = trainer.evaluate()\n",
        "eval_loss = eval_result[\"eval_loss\"]\n",
        "\n",
        "print(f\"Pérdida de validación: {eval_loss}\")\n",
        "print(f\"Perplejidad: {compute_perplexity(eval_loss)}\")\n",
        "\n",
        "# Liberar memoria al final\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "5-tlhWrKrIZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el modelo fine-tuneado\n",
        "\n",
        "Una vez finalizado el entrenamiento, guarda el modelo fine-tuneado y su tokenizador para reutilizarlos más adelante."
      ],
      "metadata": {
        "id": "4IMnE4Uwa7sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine_tuned_gpt2')\n",
        "tokenizer.save_pretrained('./fine_tuned_gpt2')"
      ],
      "metadata": {
        "id": "G_oAHXs8a7oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación y Comparación de Respuestas entre el Modelo Base y el Modelo Fine-Tuneado\n",
        "\n",
        "Esta celda implementa una función para generar respuestas del modelo y comparar los resultados entre el modelo base y el fine-tuneado. Se utiliza la misma entrada (prompt) en ambos modelos para observar cómo el fine tunning mejora (o cambia) las respuestas."
      ],
      "metadata": {
        "id": "oJ6uaXObbEOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar una respuesta\n",
        "def generate_response(prompt, model, tokenizer, max_length=50):\n",
        "    model.eval()  # Asegurarse de que el modelo esté en modo evaluación\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "    # Generar salida del modelo\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_length,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        top_p=0.8,  # Controlar diversidad\n",
        "        top_k=30,   # Opciones más probables\n",
        "        temperature=0.7,  # Controlar aleatoriedad\n",
        "        do_sample=True  # Habilitar muestreo aleatorio\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "# Cargar ambos modelos: base y fine-tuneado\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\").to(device)\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
        "\n",
        "# Comparar respuestas entre ambos modelos\n",
        "def compare_models(prompt):\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Generar respuesta con el modelo base\n",
        "    base_response = generate_response(prompt, base_model, tokenizer)\n",
        "    print(f\"Respuesta del Modelo Base:\\n{base_response}\\n\")\n",
        "\n",
        "    # Generar respuesta con el modelo fine-tuneado\n",
        "    fine_tuned_response = generate_response(prompt, fine_tuned_model, tokenizer)\n",
        "    print(f\"Respuesta del Modelo Fine-Tuneado:\\n{fine_tuned_response}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Ejemplo de comparación con un prompt personalizado\n",
        "prompt = \"Hola Nico, cómo estás? Te queria pregunar algo\"\n",
        "compare_models(prompt)\n",
        "\n",
        "# Comparar respuestas usando prompts limpios del dataset\n",
        "for i in range(2):\n",
        "    prompt = prompts[i]\n",
        "    compare_models(prompt)\n"
      ],
      "metadata": {
        "id": "yMJb56PCbEFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Interactivo con el Modelo Fine-Tuneado\n",
        "\n",
        " Acá se implementa un chat interactivo con el modelo fine-tuneado, permitiendonos conversar directamente con el chat bot. El ciclo permanece activo hasta que escribamos 'terminar', momento en el que la conversación finaliza."
      ],
      "metadata": {
        "id": "1BDzCsuUItLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bienvenido al chat con el bot. Escribe 'terminar' para finalizar la conversación.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Vos: \")  # Capturar entrada del usuario\n",
        "    if user_input.lower() == \"terminar\":\n",
        "        print(\"Bot: Hasta luego!\")\n",
        "        break  # Finaliza el chat si el usuario escribe \"terminar\"\n",
        "\n",
        "    # Generar la respuesta del bot con longitud máxima de 50 tokens\n",
        "    response = generate_response(user_input, model, tokenizer, max_length=50)\n",
        "    print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "7iZiTWNXKQYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}