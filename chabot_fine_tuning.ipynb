{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BazanNicolas/Chabot-Fine-Tuning/blob/main/chabot_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qghu8nMe45Vh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "2qB3a-IzAlcR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del chat de WhatsApp para la creaci칩n del dataset\n",
        "\n",
        "Primero debemos cargar un archivo de chat de WhatsApp (llamado chat.txt por defecto). Luego filtramos mensajes irrelevantes, como por ejemplo aquellos donde se encontraban mensajes multimedia, y separamos las conversaciones en prompts y responses. Adem치s capturamos los mensajes del autor objetivo (\"Nico Bazan\" en este caso) y los emparejamos con los mensajes de los otros participantes del chat sin importar sus nombres para usarlos como entrenamiento.\n",
        "Es importante notar que tambi칠n se verifica que los mensajes no est칠n vac칤os."
      ],
      "metadata": {
        "id": "4TFfuTBzCfrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos irrelevantes a ignorar\n",
        "irrelevantData = {\n",
        "    'Eliminaste este mensaje.',\n",
        "    'Se elimin칩 este mensaje.',\n",
        "    '<Multimedia omitido>',\n",
        "    'You deleted this message',\n",
        "    'This message was deleted',\n",
        "    '<Media omitted>'\n",
        "}\n",
        "\n",
        "# Verificar si el mensaje es irrelevante\n",
        "def containsIrrelevantData(message):\n",
        "    return any(irrelevant in message for irrelevant in irrelevantData)\n",
        "\n",
        "# Procesar el chat con m칰ltiples autores\n",
        "def process_whatsapp_chat(filepath, target_author=\"Nico Bazan\"):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        chat_lines = file.readlines()\n",
        "\n",
        "    prompts = []\n",
        "    responses = []\n",
        "\n",
        "    current_prompt = \"\"\n",
        "    current_response = \"\"\n",
        "    in_prompt = True  # Indica si estamos acumulando en el prompt\n",
        "\n",
        "    for line in chat_lines:\n",
        "        # Extraer fecha, autor y contenido del mensaje\n",
        "        match = re.match(r'\\d+/\\d+/\\d+,\\s\\d+:\\d+\\s-\\s([^:]+):\\s(.+)', line)\n",
        "\n",
        "        if match:\n",
        "            author = match.group(1).strip()\n",
        "            message = match.group(2).strip()\n",
        "\n",
        "            # Ignorar mensajes irrelevantes\n",
        "            if containsIrrelevantData(message):\n",
        "                continue\n",
        "\n",
        "            if author != target_author:  # Otros autores\n",
        "                if in_prompt:\n",
        "                    current_prompt += f\" {message}\" if current_prompt else message\n",
        "                else:\n",
        "                    # Guardar el par si no est치 vac칤o\n",
        "                    if current_prompt.strip() and current_response.strip():\n",
        "                        prompts.append(current_prompt.strip())\n",
        "                        responses.append(current_response.strip())\n",
        "                    # Reiniciar para el siguiente ciclo\n",
        "                    current_prompt = message\n",
        "                    current_response = \"\"\n",
        "                    in_prompt = True  # Volver a modo prompt\n",
        "            else:  # Mensajes del autor objetivo (Nico Bazan)\n",
        "                if not in_prompt:\n",
        "                    current_response += f\" {message}\" if current_response else message\n",
        "                else:\n",
        "                    in_prompt = False  # Cambiar a modo respuesta\n",
        "                    current_response = message\n",
        "\n",
        "    # Guardar el 칰ltimo par si no est치 vac칤o\n",
        "    if current_prompt.strip() and current_response.strip():\n",
        "        prompts.append(current_prompt.strip())\n",
        "        responses.append(current_response.strip())\n",
        "\n",
        "    return prompts, responses\n",
        "\n",
        "# Procesar el chat\n",
        "chat_file = 'chat.txt'\n",
        "target_author = 'Nico Bazan'\n",
        "prompts, responses = process_whatsapp_chat(chat_file, target_author)\n",
        "\n",
        "# Eliminar pares vac칤os\n",
        "prompts, responses = zip(*[\n",
        "    (p, r) for p, r in zip(prompts, responses) if p.strip() and r.strip()\n",
        "])\n",
        "\n",
        "# Verificar los resultados\n",
        "print(\"Ejemplo de Prompts:\", prompts[:2])\n",
        "print(\"Ejemplo de Responses:\", responses[:2])\n"
      ],
      "metadata": {
        "id": "JaaCwqA85JKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir 3 ejemplos limpios de prompts y responses\n",
        "for i in range(min(3, len(prompts))):\n",
        "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
        "    print(f\"Response {i+1}: {responses[i]}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "FQT-xZ1K-5jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creaci칩n del archivo de entrenamiento con delimitadores\n",
        "\n",
        "Convertiremos los prompts y responses procesados en un formato estructurado utilizando delimitadores claros (PROMPT, RESPONSE y ###). Cada par de conversaci칩n se guarda en el archivo train_data.txt para su posterior uso en el entrenamiento del modelo."
      ],
      "metadata": {
        "id": "x3tRuvFuD2U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear archivo con delimitadores claros para prompt y response\n",
        "train_data = \"\"\n",
        "for prompt, response in zip(prompts, responses):\n",
        "    train_data += f\"PROMPT:\\n{prompt}\\nRESPONSE:\\n{response}\\n###\\n\"  # Delimitadores claros\n",
        "\n",
        "# Guardar los datos en 'train_data.txt'\n",
        "with open('train_data.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(train_data)\n",
        "\n",
        "print(\"Datos guardados correctamente en 'train_data.txt'.\")\n"
      ],
      "metadata": {
        "id": "HAI5oVsR_AGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y configuraci칩n del modelo GPT-2 en espa침ol\n",
        "\n",
        "Ahora realizamos la configuraci칩n del modelo GPT-2 en espa침ol que ser치 fine tuneado utilizando la librer칤a transformers. Adem치s, se ajustan par치metros como:\n",
        "\n",
        "* Carga del modelo base y el tokenizador GPT-2 con un model_max_length de 64.\n",
        "* Configuraci칩n del dropout (comentada inicialmente, pero se puede ajustar para el entrenamiento).\n",
        "* Alineaci칩n del pad_token con `eos_token,** garantizando que el padding no interfiera con el procesamiento del modelo.\n",
        "* Ajuste de embeddings: Se asegura de que la dimensi칩n del vocabulario se adapte al tokenizador cargado.\n",
        "* Sincronizaci칩n del peso de la capa de salida (lm_head) con los embeddings iniciales para evitar inconsistencias."
      ],
      "metadata": {
        "id": "stPFUorYcD2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xYrPX5s0QFEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cargar el modelo base en espa침ol\n",
        "model_name = \"DeepESP/gpt2-spanish\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=64)\n",
        "\n",
        "# Configurar dropout (esto se aplicar치 durante el entrenamiento)\n",
        "# model.config.attn_pdrop = 0.3\n",
        "# model.config.embd_pdrop = 0.3\n",
        "# model.config.resid_pdrop = 0.3\n",
        "\n",
        "\n",
        "# Alinear el token de padding con el token de fin de secuencia\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))  # Ajustar embeddings\n",
        "\n",
        "# Inicializa el peso de la lm_head si es necesario\n",
        "with torch.no_grad():\n",
        "    model.lm_head.weight = model.transformer.wte.weight\n"
      ],
      "metadata": {
        "id": "U5XXYZhjcDx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y creaci칩n del dataset para entrenamiento y validaci칩n\n",
        "\n",
        "Esta celda carga los datos desde el archivo train_data.txt, los procesa utilizando delimitadores claros y los convierte en un dataset compatible con Hugging Face. A continuaci칩n, el dataset se divide en subconjuntos para entrenamiento y validaci칩n (80-20), garantizando que los datos se mezclen aleatoriamente."
      ],
      "metadata": {
        "id": "sbq8YqfWaPU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Funci칩n para leer el archivo y agrupar los ejemplos con delimitadores claros\n",
        "def load_grouped_messages(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    # Usar regex para extraer prompts y responses con delimitadores claros\n",
        "    examples = re.findall(r\"PROMPT:\\n(.*?)\\nRESPONSE:\\n(.*?)\\n###\", raw_data, re.DOTALL)\n",
        "\n",
        "    if not examples:\n",
        "        raise ValueError(\"No se encontraron ejemplos en el archivo. Verifica el formato.\")\n",
        "\n",
        "    # Crear lista de diccionarios con prompts y responses separados\n",
        "    data = [{\"prompt\": prompt.strip(), \"response\": response.strip()} for prompt, response in examples]\n",
        "    return data\n",
        "\n",
        "# Cargar los datos desde el archivo\n",
        "data = load_grouped_messages(\"train_data.txt\")\n",
        "\n",
        "# Crear un Dataset de Huggingface a partir de los datos cargados\n",
        "dataset = Dataset.from_dict({\n",
        "    \"prompt\": [d[\"prompt\"] for d in data],\n",
        "    \"response\": [d[\"response\"] for d in data]\n",
        "})\n",
        "\n",
        "# Dividir el dataset en entrenamiento y validaci칩n, asegurando mezcla\n",
        "train_test_split = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
        "\n",
        "# Crear un DatasetDict para entrenamiento y validaci칩n\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_test_split[\"train\"],\n",
        "    \"validation\": train_test_split[\"test\"]\n",
        "})\n",
        "\n",
        "# Verificar la estructura del dataset\n",
        "print(\"Primeros ejemplos de entrenamiento:\\n\", datasets[\"train\"][:2])\n"
      ],
      "metadata": {
        "id": "CynPKzyBOBpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definici칩n de funciones auxiliares para tokenizaci칩n y agrupaci칩n de textos\n",
        "\n",
        "Esta celda define dos funciones auxiliares esenciales para la preparaci칩n del dataset y el entrenamiento del modelo. Estas funciones se encargan de tokenizar los textos y agruparlos en bloques del tama침o adecuado para optimizar el entrenamiento del modelo.\n",
        "Devuelven un nuevo dataset en el que los textos est치n tokenizados (convertidos en 칤ndices del vocabulario) y se incluye una m치scara de atenci칩n para distinguir los tokens relevantes de los tokens de padding. Adem치s, se preparan labels para el entrenamiento.\n",
        "\n",
        "(Cr칠ditos a [Cristian Cardellino](https://crscardellino.net) por estas [funciones](https://huggingface.co/crscardellino/flisol-cba-martin-fierro/resolve/main/utils.py))"
      ],
      "metadata": {
        "id": "xH7h4NSyFQFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "from typing import Callable, Dict, List\n",
        "\n",
        "\n",
        "def tokenize(\n",
        "    tokenizer: PreTrainedTokenizerBase, end_char: str = \"\\n\"\n",
        ") -> Callable[[Dict[str, List[str]]], DatasetDict]:\n",
        "    \"\"\"\n",
        "    Tokeniza los textos agregando un caracter final opcional (`end_char`).\n",
        "    \"\"\"\n",
        "\n",
        "    def _tokenize(examples: Dict[str, List[str]]) -> DatasetDict:\n",
        "        # Asegurarse de que cada entrada tiene texto v치lido\n",
        "        texts = [f\"{e}{end_char}\" for e in examples[\"text\"] if e.strip()]\n",
        "        return tokenizer(texts, truncation=True, padding=True)\n",
        "\n",
        "    return _tokenize\n",
        "\n",
        "\n",
        "def group_texts(examples: Dict[str, List[int]], block_size: int = 128) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Agrupa los textos en bloques del tama침o especificado y genera etiquetas (labels).\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenar todos los tokens\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[\"input_ids\"])\n",
        "\n",
        "    if total_length < block_size:\n",
        "        raise ValueError(\"Los datos son insuficientes para formar un bloque del tama침o especificado.\")\n",
        "\n",
        "    # Ajustar el total_length para evitar remainders incompletos\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # Dividir en bloques del tama침o especificado\n",
        "    result = {\n",
        "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "    # Copiar input_ids como labels para el entrenamiento\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "oqCeFuouBzLu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizaci칩n del dataset para preparaci칩n del entrenamiento\n",
        "\n",
        "En esta celda se realiza la tokenizaci칩n de los datos de entrenamiento y validaci칩n, preparando los textos para que puedan ser procesados por el modelo. La funci칩n de tokenizaci칩n transforma los textos en 칤ndices del vocabulario y a침ade padding para mantener la consistencia en la longitud de los ejemplos."
      ],
      "metadata": {
        "id": "1fpw9SB0ahhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "'''\n",
        "Prop칩sito: Combina los prompts y responses en un solo string, los tokeniza y asegura que todos los ejemplos tengan la misma longitud (128 tokens).\n",
        "Truncamiento: Corta los textos si superan el l칤mite definido (128 tokens).\n",
        "Padding: A침ade tokens de relleno hasta alcanzar 128 tokens para garantizar que todos los ejemplos tengan la misma longitud.\n",
        "Retorno: Devuelve los tensores en formato PyTorch (return_tensors=\"pt\").\n",
        "'''\n",
        "def tokenize_function(examples):\n",
        "    # Concatenar prompt y response en un solo string\n",
        "    texts = [f\"{p} {r}\".strip() for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,  # Truncar si excede el l칤mite\n",
        "        max_length=128,    # Limitar a 128 tokens\n",
        "        padding=\"max_length\",  # Rellenar hasta 128 tokens\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Usar partial para fijar el tokenizador en la funci칩n\n",
        "tokenize_with_tokenizer = partial(tokenize_function)\n",
        "\n",
        "# Aplicar la tokenizaci칩n al dataset usando el partial\n",
        "tokenized_datasets = datasets.map(\n",
        "    tokenize_with_tokenizer,  # Usar la funci칩n parcial\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=[\"prompt\", \"response\"]  # Remover columnas originales\n",
        ")\n",
        "\n",
        "# Verificar los primeros datos tokenizados\n",
        "print(\"Datos tokenizados:\\n\", tokenized_datasets[\"train\"][:2])\n"
      ],
      "metadata": {
        "id": "hBzd_JUSahbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agrupaci칩n de textos en bloques del tama침o especificado\n",
        "\n",
        "Se aplica la funci칩n group_texts al dataset previamente tokenizado para dividir los tokens en bloques del tama침o especificado (por defecto, 128 tokens). Esto asegura que los lotes (batches) tengan una longitud uniforme durante el entrenamiento."
      ],
      "metadata": {
        "id": "2K0JJOHbG2AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar los textos en bloques del tama침o especificado\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=1  # Cambiar a 1 si est치s usando Google Colab sin n칰cleos adicionales\n",
        ")\n",
        "\n",
        "# Verificar los primeros ejemplos agrupados (mostrar solo los primeros 20 tokens)\n",
        "print(\"Datos agrupados (primeros 20 tokens):\\n\",\n",
        "      {k: v[:20] for k, v in lm_datasets[\"train\"][:2].items()})\n"
      ],
      "metadata": {
        "id": "U58Fx_77CPn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuraci칩n del entrenamiento y optimizaci칩n del modelo\n",
        "\n",
        "Se configura los par치metros clave para el entrenamiento del modelo mediante el uso de TrainingArguments y el objeto Trainer. Se define el optimizador AdamW con un scheduler cosine para ajustar din치micamente la tasa de aprendizaje. Adem치s, se calcula el n칰mero de pasos totales y de calentamiento (warmup) para controlar la tasa al inicio del entrenamiento. La configuraci칩n incluye la opci칩n de EarlyStopping para detener el entrenamiento si no hay mejora en la p칠rdida, optimizando as칤 los recursos."
      ],
      "metadata": {
        "id": "6tLNtlNlavWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
        "\n",
        "# Argumentos del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_gpt2\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluar despu칠s de cada 칠poca\n",
        "    save_strategy=\"epoch\",  # Guardar el modelo al final de cada 칠poca\n",
        "    num_train_epochs=3,  # N칰mero de 칠pocas\n",
        "    per_device_train_batch_size=8,  # Tama침o del batch\n",
        "    learning_rate=1e-5,  # Tasa de aprendizaje\n",
        "    weight_decay=0.01,  # Regularizaci칩n\n",
        "    logging_steps=5,  # Log cada 5 pasos\n",
        "    report_to=[],  # Deshabilitar W&B\n",
        "    load_best_model_at_end=True,  # Cargar el mejor modelo al final\n",
        "    metric_for_best_model=\"loss\",  # Usar 'loss' como m칠trica principal\n",
        "    greater_is_better=False  # Minimizar la p칠rdida\n",
        ")\n",
        "\n",
        "# Definir el optimizador usando los argumentos\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "# Calcular el n칰mero total de pasos de entrenamiento\n",
        "num_training_steps = (\n",
        "    len(lm_datasets[\"train\"]) // training_args.per_device_train_batch_size\n",
        ") * training_args.num_train_epochs\n",
        "\n",
        "num_warmup_steps = int(num_training_steps * 0.1)  # Warmup del 10%\n",
        "scheduler = get_scheduler(\n",
        "    \"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=int(num_training_steps * 0.1),\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "\n",
        "# Configurar el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        "    optimizers=(optimizer, scheduler),  # Optimizer y scheduler\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Detener si no mejora\n",
        ")\n"
      ],
      "metadata": {
        "id": "u1F_02_vavR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicio del Entrenamiento del Modelo\n",
        "Ejecuta el proceso de fine-tuning"
      ],
      "metadata": {
        "id": "oz4XNjNoa1D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sMXpnuNsa1AP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8fdb66e6-2661-4069-f4f2-a3931176d2a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6699' max='6699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6699/6699 38:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.161100</td>\n",
              "      <td>1.051037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.189500</td>\n",
              "      <td>1.019949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.912100</td>\n",
              "      <td>1.016659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6699, training_loss=1.0928075167292497, metrics={'train_runtime': 2334.4003, 'train_samples_per_second': 22.952, 'train_steps_per_second': 2.87, 'total_flos': 3500006768640000.0, 'train_loss': 1.0928075167292497, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluaci칩n del Modelo: Perplejidad y Entrop칤a Cruzada (No ejecutar por ahora, Cuda se queda sin memoria 游땴)\n",
        "\n",
        "Esta celda eval칰a el rendimiento del modelo mediante dos m칠tricas: perplejidad y entrop칤a cruzada. La perplejidad se calcula a partir de la p칠rdida en el conjunto de validaci칩n y refleja qu칠 tan bien el modelo predice secuencias de texto. La entrop칤a cruzada mide la discrepancia entre las distribuciones de los logits del modelo(predicciones sin procesar) y las etiquetas reales.\n",
        "En otras palabras, mide cu치n diferentes son estas dos distribuciones: la predicci칩n del modelo (logits) y la distribuci칩n \"objetivo\" (la palabra correcta con probabilidad 1). Una"
      ],
      "metadata": {
        "id": "Z8ZMltKSHw07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def compute_perplexity(eval_loss):\n",
        "    \"\"\"Calcula la perplejidad a partir de la p칠rdida.\"\"\"\n",
        "    return math.exp(eval_loss)\n",
        "\n",
        "def compute_cross_entropy(logits, labels):\n",
        "    \"\"\"Calcula la entrop칤a cruzada entre los logits y las etiquetas reales.\"\"\"\n",
        "    shift_logits = logits[..., :-1, :].contiguous()  # Desplazar logits\n",
        "    shift_labels = labels[..., 1:].contiguous()  # Desplazar etiquetas\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1)\n",
        "    )\n",
        "    return loss.item()\n",
        "\n",
        "# Liberar memoria ANTES de la evaluaci칩n\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Evaluar el modelo con batches peque침os para evitar problemas de memoria\n",
        "valid_dataset = lm_datasets[\"validation\"]\n",
        "batch_size = 2  # Ajustar seg칰n la memoria disponible\n",
        "\n",
        "logits_list = []\n",
        "labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(valid_dataset), batch_size):\n",
        "        batch = valid_dataset[i: i + batch_size]\n",
        "\n",
        "        # Preparar inputs y moverlos a GPU\n",
        "        inputs = {\n",
        "            \"input_ids\": torch.tensor(batch[\"input_ids\"]).to(device),\n",
        "            \"attention_mask\": torch.tensor(batch[\"attention_mask\"]).to(device),\n",
        "        }\n",
        "        labels = torch.tensor(batch[\"labels\"]).to(device)\n",
        "\n",
        "        # Generar logits y moverlos inmediatamente a la CPU para liberar memoria GPU\n",
        "        outputs = model(**inputs)\n",
        "        logits_list.append(outputs.logits.cpu())\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "        # Liberar memoria GPU despu칠s de cada lote\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# Concatenar todos los logits y etiquetas\n",
        "logits = torch.cat(logits_list, dim=0)\n",
        "all_labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "# Calcular la entrop칤a cruzada con los logits concatenados\n",
        "cross_entropy = compute_cross_entropy(logits, all_labels)\n",
        "print(f\"Entrop칤a Cruzada: {cross_entropy}\")\n",
        "\n",
        "# Evaluar el modelo usando el Trainer y mostrar la perplejidad\n",
        "eval_result = trainer.evaluate()\n",
        "eval_loss = eval_result[\"eval_loss\"]\n",
        "\n",
        "print(f\"P칠rdida de validaci칩n: {eval_loss}\")\n",
        "print(f\"Perplejidad: {compute_perplexity(eval_loss)}\")\n",
        "\n",
        "# Liberar memoria al final\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "5-tlhWrKrIZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el modelo fine-tuneado\n",
        "\n",
        "Una vez finalizado el entrenamiento, guarda el modelo fine-tuneado y su tokenizador para reutilizarlos m치s adelante."
      ],
      "metadata": {
        "id": "4IMnE4Uwa7sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine_tuned_gpt2')\n",
        "tokenizer.save_pretrained('./fine_tuned_gpt2')"
      ],
      "metadata": {
        "id": "G_oAHXs8a7oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generaci칩n y Comparaci칩n de Respuestas entre el Modelo Base y el Modelo Fine-Tuneado\n",
        "\n",
        "Esta celda implementa una funci칩n para generar respuestas del modelo y comparar los resultados entre el modelo base y el fine-tuneado. Se utiliza la misma entrada (prompt) en ambos modelos para observar c칩mo el fine tunning mejora (o cambia) las respuestas."
      ],
      "metadata": {
        "id": "oJ6uaXObbEOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci칩n para generar una respuesta\n",
        "def generate_response(prompt, model, tokenizer, max_length=50):\n",
        "    model.eval()  # Asegurarse de que el modelo est칠 en modo evaluaci칩n\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "    # Generar salida del modelo\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_length,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        top_p=0.8,  # Controlar diversidad\n",
        "        top_k=30,   # Opciones m치s probables\n",
        "        temperature=0.7,  # Controlar aleatoriedad\n",
        "        do_sample=True  # Habilitar muestreo aleatorio\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "# Cargar ambos modelos: base y fine-tuneado\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\").to(device)\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
        "\n",
        "# Comparar respuestas entre ambos modelos\n",
        "def compare_models(prompt):\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Generar respuesta con el modelo base\n",
        "    base_response = generate_response(prompt, base_model, tokenizer)\n",
        "    print(f\"Respuesta del Modelo Base:\\n{base_response}\\n\")\n",
        "\n",
        "    # Generar respuesta con el modelo fine-tuneado\n",
        "    fine_tuned_response = generate_response(prompt, fine_tuned_model, tokenizer)\n",
        "    print(f\"Respuesta del Modelo Fine-Tuneado:\\n{fine_tuned_response}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Ejemplo de comparaci칩n con un prompt personalizado\n",
        "prompt = \"Hola Nico, c칩mo est치s? Te queria pregunar algo\"\n",
        "compare_models(prompt)\n",
        "\n",
        "# Comparar respuestas usando prompts limpios del dataset\n",
        "for i in range(2):\n",
        "    prompt = prompts[i]\n",
        "    compare_models(prompt)\n"
      ],
      "metadata": {
        "id": "yMJb56PCbEFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Interactivo con el Modelo Fine-Tuneado\n",
        "\n",
        " Ac치 se implementa un chat interactivo con el modelo fine-tuneado, permitiendonos conversar directamente con el chat bot. El ciclo permanece activo hasta que escribamos 'terminar', momento en el que la conversaci칩n finaliza."
      ],
      "metadata": {
        "id": "1BDzCsuUItLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bienvenido al chat con el bot. Escribe 'terminar' para finalizar la conversaci칩n.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Vos: \")  # Capturar entrada del usuario\n",
        "    if user_input.lower() == \"terminar\":\n",
        "        print(\"Bot: Hasta luego!\")\n",
        "        break  # Finaliza el chat si el usuario escribe \"terminar\"\n",
        "\n",
        "    # Generar la respuesta del bot con longitud m치xima de 50 tokens\n",
        "    response = generate_response(user_input, model, tokenizer, max_length=50)\n",
        "    print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "7iZiTWNXKQYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}