{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9145586-0d3b-478c-94dc-a595a61b5573",
   "metadata": {},
   "source": [
    "## Instalaci√≥n de dependencias y configuraci√≥n del entorno\n",
    "\n",
    "Instalamos todas las librer√≠as necesarias para el procesamiento de datos, entrenamiento y evaluaci√≥n de modelos de lenguaje. Incluye utilidades para embeddings sem√°nticos, visualizaci√≥n, PyTorch con soporte CUDA y el ecosistema completo de Hugging Face, adem√°s de TRL desde GitHub para asegurar compatibilidad con configuraciones avanzadas de fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623398-c458-4703-9398-012d7d2dba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utilities required for dataset handling, semantic embeddings, and visualization\n",
    "%pip install sentence_transformers datasets matplotlib seaborn tf-keras\n",
    "\n",
    "# Core deep learning stack (PyTorch with CUDA support on Colab)\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# Hugging Face ecosystem for model loading, training, and quantization\n",
    "%pip install -U transformers accelerate peft bitsandbytes\n",
    "\n",
    "# helpers\n",
    "%pip install wordcloud\n",
    "\n",
    "# TRL from GitHub to ensure compatibility with the latest SFTConfig features\n",
    "%pip install git+https://github.com/huggingface/trl.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f07b3-5d30-403c-9b93-5ee4ba385a9f",
   "metadata": {},
   "source": [
    "## Configuraci√≥n del entorno de ejecuci√≥n (GPU y Accelerate)\n",
    "\n",
    "Establecemos variables de entorno para controlar el uso de GPU y evitar problemas comunes al entrenar modelos en notebooks. En particular, se fuerza el uso de una √∫nica GPU y se desactiva el mixed precision de Accelerate para mejorar la estabilidad del entrenamiento en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb154d96-0d93-48fd-8b7e-af0009d50fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Restrict execution to a single GPU to avoid DataParallel-related issues in notebooks\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Disable Accelerate mixed precision for improved stability in this environment\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932d628-6f07-4fa5-b392-29b94686ef3b",
   "metadata": {},
   "source": [
    "## Importaci√≥n de librer√≠as\n",
    "\n",
    "Cargamos todas las dependencias necesarias para el procesamiento del chat, construcci√≥n del dataset, entrenamiento (Transformers/TRL), cuantizaci√≥n (bitsandbytes), LoRA (PEFT), evaluaci√≥n y visualizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6272e-24a2-4de5-b229-f904dd785e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from IPython.display import clear_output, display\n",
    "from peft import LoraConfig, PeftModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83ac45-92be-43b7-b179-e20a24e47f15",
   "metadata": {},
   "source": [
    "## Configuraci√≥n del dispositivo de ejecuci√≥n\n",
    "\n",
    "Establecemos el dispositivo de c√≥mputo a utilizar durante el entrenamiento y la inferencia. Utilizamos la GPU si est√° disponible, de lo contrario el c√≥digo se ejecuta en CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451d6b4-c471-4b56-af51-9027b8d7964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        cap = torch.cuda.get_device_capability(i)\n",
    "        print(f\"  GPU {i}: {name} ‚Äî Compute Capability: {cap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46aa9-24ac-440f-bb22-aa891afd14c0",
   "metadata": {},
   "source": [
    "## Constantes globales del experimento\n",
    "\n",
    "Definimos las constantes fijas del experimento y validamos que la configuraci√≥n interactiva haya sido ejecutada correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a32065-7f53-43ba-8eb0-ee67ad5b669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed model identifier\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# Special token used to separate consecutive messages\n",
    "MSG_SEP = \"<|msg_sep|>\"\n",
    "\n",
    "# Safety check: interactive configuration must be executed\n",
    "required_vars = [\"chat_file\", \"target_author\", \"bot_name\", \"OUTPUT_DIR\"]\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Falta ejecutar la configuraci√≥n interactiva. \"\n",
    "        f\"Variables no definidas: {missing}\"\n",
    "    )\n",
    "\n",
    "print(\"Constantes globales cargadas correctamente\")\n",
    "print(\"Modelo:\", MODEL_ID)\n",
    "print(\"Autor objetivo:\", target_author)\n",
    "print(\"Bot:\", bot_name)\n",
    "print(\"Directorio de salida:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a601590-f22d-4ed7-9823-a98f81bbfe0f",
   "metadata": {},
   "source": [
    "## Configuraci√≥n interactiva del chat\n",
    "\n",
    "Ahora subimos el archivo de chat, el autor objetivo y el nombre del bot. Esta configuraci√≥n es obligatoria y debe ejecutarse antes de continuar con el notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449617c-2778-47b3-ba61-d2a619beb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader = widgets.FileUpload(\n",
    "    accept=\".txt\",\n",
    "    multiple=False,\n",
    "    description=\"Subir chat (.txt)\"\n",
    ")\n",
    "\n",
    "author_input = widgets.Text(\n",
    "    description=\"Autor:\",\n",
    "    placeholder=\"Exactamente como aparece en el chat\",\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "bot_input = widgets.Text(\n",
    "    description=\"Bot:\",\n",
    "    placeholder=\"Nombre del bot (ej: MELINA)\",\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "apply_button = widgets.Button(\n",
    "    description=\"Aplicar configuraci√≥n\",\n",
    "    button_style=\"primary\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_apply_clicked(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "\n",
    "        if len(uploader.value) == 0:\n",
    "            print(\"Por favor, sub√≠ un archivo .txt del chat.\")\n",
    "            return\n",
    "\n",
    "        author = author_input.value.strip()\n",
    "        bot = bot_input.value.strip()\n",
    "\n",
    "        if not author:\n",
    "            print(\"Ingres√° el autor exactamente como aparece en el chat.\")\n",
    "            return\n",
    "        if not bot:\n",
    "            print(\"Ingres√° un nombre para el bot.\")\n",
    "            return\n",
    "\n",
    "        upload_info = uploader.value[0]\n",
    "        content = bytes(upload_info[\"content\"]).decode(\"utf-8-sig\", errors=\"replace\")\n",
    "\n",
    "        global chat_file, target_author, bot_name, OUTPUT_DIR\n",
    "\n",
    "        chat_file = \"uploaded_chat.txt\"\n",
    "        with open(chat_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        target_author = author\n",
    "        bot_name = bot\n",
    "        OUTPUT_DIR = \"./chatbot_\" + bot_name\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        print(\"Configuraci√≥n aplicada correctamente\")\n",
    "        print(\"chat_file:\", chat_file)\n",
    "        print(\"target_author:\", target_author)\n",
    "        print(\"bot_name:\", bot_name)\n",
    "        print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "apply_button.on_click(on_apply_clicked)\n",
    "\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Configuraci√≥n interactiva del chat</b>\"),\n",
    "        uploader,\n",
    "        author_input,\n",
    "        bot_input,\n",
    "        apply_button,\n",
    "        out\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6687fe-9f74-4262-9c89-e1964f13bb63",
   "metadata": {},
   "source": [
    "## Par√°metros de filtrado y mensajes irrelevantes\n",
    "\n",
    "Definimos el conjunto de patrones que identifican mensajes autom√°ticos o irrelevantes generados por WhatsApp (por ejemplo, avisos del sistema o contenido multimedia omitido). Estos patrones se utilizan para filtrar el chat antes de construir los ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e028e9-40b1-4b28-854a-d52d27791a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantData = {\n",
    "    # Spanish system messages\n",
    "    'eliminaste este mensaje',\n",
    "    'se elimin√≥ este mensaje',\n",
    "    '<multimedia omitido>',\n",
    "    'multimedia omitido',\n",
    "    'los mensajes y las llamadas est√°n cifrados de extremo a extremo',\n",
    "    # English system messages\n",
    "    'you deleted this message',\n",
    "    'this message was deleted',\n",
    "    '<media omitted>',\n",
    "    'media omitted',\n",
    "    'messages and calls are end-to-end encrypted',\n",
    "}\n",
    "\n",
    "def containsIrrelevantData(message: str) -> bool:\n",
    "    \"\"\"\n",
    "    Assumes the input message is already lowercased.\n",
    "    Returns True if the message contains any known WhatsApp system or noise pattern.\n",
    "    \"\"\"\n",
    "    msg = message.lower()\n",
    "    return any(irr in msg for irr in irrelevantData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da54b6-a85b-4d3b-a572-f813cd000f05",
   "metadata": {},
   "source": [
    "## Tokenizer del modelo\n",
    "\n",
    "Cargamos el tokenizer asociado al modelo base. Se utiliza durante el preprocesamiento para convertir el historial de conversaci√≥n al formato exacto requerido por el chat template del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9069-0174-437a-ba7f-ea83cfba6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a48ffc-84f6-42cb-b17e-eefd94a5e5cd",
   "metadata": {},
   "source": [
    "## Procesamiento del chat de WhatsApp y formateo para el modelo\n",
    "\n",
    "Convertimos el chat exportado de WhatsApp en ejemplos de entrenamiento listos para fine-tuning. Se agrupan mensajes consecutivos por autor, se construye un historial acotado por k_history y time_gap, y finalmente se serializa cada ejemplo usando el chat_template del tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5866af4-710a-4a1b-8936-29bc96621cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light text cleaning: lowercasing, whitespace normalization, and basic character filtering.\n",
    "    Noise filtering (WhatsApp system messages) is handled by `containsIrrelevantData`.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º0-9,.;:¬°!¬ø?\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def parse_datetime(line: str):\n",
    "    \"\"\"Extracts WhatsApp timestamp if present; returns datetime or None.\"\"\"\n",
    "    match = re.match(r\"(\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+)\\s-\", line)\n",
    "    if match:\n",
    "        for fmt in (\"%d/%m/%y %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
    "            try:\n",
    "                return datetime.strptime(match.group(1).replace(\",\", \"\"), fmt)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def group_consecutive_messages(messages):\n",
    "    \"\"\"\n",
    "    Groups consecutive messages from the same author into a single turn if they are close in time.\n",
    "    Multiple messages within the same turn are joined using MSG_SEP.\n",
    "    \"\"\"\n",
    "    grouped = []\n",
    "    for author, msg, ts in messages:\n",
    "        if (\n",
    "            grouped\n",
    "            and grouped[-1][0] == author\n",
    "            and ts and grouped[-1][2]\n",
    "            and (ts - grouped[-1][2]) < timedelta(hours=1)\n",
    "        ):\n",
    "            # Same author within the time window -> merge into the previous turn\n",
    "            prev_msg = grouped[-1][1]\n",
    "            new_msg = prev_msg + f\" {MSG_SEP} \" + msg\n",
    "            grouped[-1] = (author, new_msg, ts)\n",
    "        else:\n",
    "            grouped.append((author, msg, ts))\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def process_whatsapp_chat_with_roles(\n",
    "    filepath,\n",
    "    k_history=4,\n",
    "    time_gap=timedelta(hours=3),\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds training samples from a WhatsApp export:\n",
    "      - Turns are consecutive messages from the same author (grouped)\n",
    "      - A backward context window is built using k_history and time_gap\n",
    "      - Roles follow the standard chat format: user / assistant\n",
    "      - The final text is serialized using tokenizer.apply_chat_template(...)\n",
    "    \"\"\"\n",
    "    print(\"Procesando chat (k-turns con roles)...\")\n",
    "\n",
    "    messages = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            ts = parse_datetime(line)\n",
    "            match = re.match(r'\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+\\s-\\s([^:]+):\\s(.+)', line)\n",
    "            if match:\n",
    "                author = match.group(1).strip()\n",
    "                raw_msg = match.group(2)\n",
    "                msg = clean_text(raw_msg)\n",
    "                # Skip empty messages and WhatsApp system/noise lines\n",
    "                if msg and not containsIrrelevantData(msg):\n",
    "                    messages.append((author, msg, ts))\n",
    "\n",
    "    if not messages:\n",
    "        print(\"No se encontraron mensajes v√°lidos.\")\n",
    "        return [], []\n",
    "\n",
    "    messages = group_consecutive_messages(messages)\n",
    "    print(f\"Total turnos agrupados: {len(messages)}\")\n",
    "\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(1, len(messages)):\n",
    "        author_i, msg_i, ts_i = messages[i]\n",
    "\n",
    "        # Only keep samples where the target author is the assistant\n",
    "        if author_i != target_author:\n",
    "            continue\n",
    "\n",
    "        conversation_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener \"\n",
    "                    f\"conversaciones casuales. Respondes con la misma personalidad que {target_author} \"\n",
    "                    f\"tiene en los chats de WhatsApp./no_think\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        temp_history = []\n",
    "        last_ts = ts_i\n",
    "\n",
    "        for j in range(i - 1, -1, -1):\n",
    "            a_j, m_j, ts_j = messages[j]\n",
    "            if ts_j and last_ts and (last_ts - ts_j) > time_gap:\n",
    "                break\n",
    "\n",
    "            # Map WhatsApp authors to standard chat roles\n",
    "            role = \"assistant\" if a_j == target_author else \"user\"\n",
    "            temp_history.insert(0, {\"role\": role, \"content\": m_j})\n",
    "\n",
    "            last_ts = ts_j if ts_j is not None else last_ts\n",
    "            if len(temp_history) >= k_history:\n",
    "                break\n",
    "\n",
    "        if not temp_history:\n",
    "            continue\n",
    "\n",
    "        conversation_history.extend(temp_history)\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": msg_i})\n",
    "\n",
    "        # Serialize messages into the exact format required by the model's chat template\n",
    "        final_text = tokenizer.apply_chat_template(\n",
    "            conversation_history,\n",
    "            tokenize=False,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "\n",
    "        formatted_data.append(final_text)\n",
    "\n",
    "    print(f\"Se generaron {len(formatted_data)} ejemplos de entrenamiento.\")\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "formatted_data = process_whatsapp_chat_with_roles(chat_file)\n",
    "df = pd.DataFrame({\"text\": formatted_data})\n",
    "df.to_json(\n",
    "    f\"{OUTPUT_DIR}/formatted_data.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Archivo guardado en: {OUTPUT_DIR}/formatted_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ab835-08c2-42af-9ea3-fc307062c2b9",
   "metadata": {},
   "source": [
    "## Preparaci√≥n y partici√≥n del dataset\n",
    "\n",
    "Aplicamos un filtrado m√≠nimo para eliminar entradas vac√≠as o ruidosas (incluyendo URLs), y convertimos el resultado a un Dataset de Hugging Face. Finalmente, dividimos el conjunto en entrenamiento y validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa2ad9-122c-4366-a492-36edbcea7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(f\"{OUTPUT_DIR}/formatted_data.jsonl\", lines=True)\n",
    "\n",
    "# Basic cleanup: drop empty or extremely short samples\n",
    "data = data[data[\"text\"].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "# URL filtering (original logic)\n",
    "data = data[~data[\"text\"].str.contains(r\"http|www|\\.com\", regex=True)]\n",
    "\n",
    "print(f\"Dataset listo para TRL: {len(data)} conversaciones.\")\n",
    "\n",
    "# Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Train/validation split\n",
    "dataset = dataset.train_test_split(test_size=0.1) # 10% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5d317-9de9-4c14-bf66-6b5472db12dc",
   "metadata": {},
   "source": [
    "## Carga del modelo y configuraci√≥n de LoRA\n",
    "\n",
    "Configuramos la cuantizaci√≥n en 4 bits para reducir el consumo de memoria, cargamos el modelo base en GPU y preparamos la adaptaci√≥n mediante LoRA. Este enfoque permite entrenar √∫nicamente un subconjunto reducido de par√°metros, haciendo el fine-tuning m√°s eficiente sin modificar los pesos originales del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60446b-ad4f-4e26-bf6a-1e642c35c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Cargando modelo {MODEL_ID} en 4-bits...\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",  # disabled to control device placement explicitly\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Tokenizer was loaded previously\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Common fix for LLaMA/SmolLM-style models\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) configuration\n",
    "# Only a small subset of parameters will be trained\n",
    "peft_config = LoraConfig(\n",
    "    r=16,        # Attention rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Trainable modules\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Memoria del modelo: {model.dtype}\")\n",
    "print(f\"Footprint de memoria: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979940f-926a-46ff-96eb-6898d453a2fb",
   "metadata": {},
   "source": [
    "## Configuraci√≥n del entrenador SFT\n",
    "\n",
    "Definimos los hiperpar√°metros de entrenamiento. Se pueden ajustan algunos como √©pocas, batch size efectivo y tasa de aprendizaje, priorizando estabilidad y uso eficiente de memoria. Finalmente, inicializamos el SFTTrainer, que integra el modelo, los datos y la configuraci√≥n LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ced775-8ba0-41a0-b9f8-bfa9d0a6eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    # Evaluation disabled to avoid OOM due to logits accumulation\n",
    "    eval_strategy=\"no\",\n",
    "\n",
    "    save_total_limit=2,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Checkpointing and logging\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673e4ed-125d-4259-b32b-3614c26f7272",
   "metadata": {},
   "source": [
    "## Entrenamiento y guardado del modelo\n",
    "\n",
    "Ejecutamos el proceso de fine-tuning utilizando SFTTrainer y, una vez finalizado, liberamos memoria de GPU para evitar fragmentaci√≥n. Finalmente, guardamos el modelo entrenado y el tokenizer en el directorio de salida definido previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a7891-4346-4a99-b32a-0871f91866be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Iniciando entrenamiento con SFTTrainer...\\n\"\n",
    "    f\"El modelo ser√° guardado en {OUTPUT_DIR}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "finally:\n",
    "    # Ensure GPU memory is released even if training is interrupted\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save trained model and tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b17843-d244-48de-ad0b-9312f91fac2e",
   "metadata": {},
   "source": [
    "## Compresi√≥n del modelo entrenado\n",
    "\n",
    "Empaquetamos el directorio de salida en un archivo .zip para facilitar su descarga y almacenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7895b1-d24f-4f93-84d1-9d5047780082",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_zip = bot_name + \"_compressed.zip\"\n",
    "\n",
    "# Compress the output directory into a ZIP archive\n",
    "!zip -r {file_zip} {OUTPUT_DIR}\n",
    "\n",
    "print(\"‚ö† Luego de que la carpeta se haya comprimido, no te olvides de descargarla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60ea94-ec2a-4efe-82e7-aab8183824de",
   "metadata": {},
   "source": [
    "## Restauraci√≥n del modelo entrenado\n",
    "\n",
    "Descomprimimos el archivo y restauramos el modelo sin necesidad de volver a ejecutar todo el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0173c2-4463-4fe5-9f97-b61f6a6e80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_zip = bot_name + \"_compressed.zip\"\n",
    "# To avoid rerunning the full pipeline, upload the previously generated ZIP\n",
    "# and execute the following command to restore the directory\n",
    "!unzip {file_zip}\n",
    "\n",
    "print(\"‚úÖ Carpeta descomprimida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ece9-b6eb-4426-9da6-dbc31355264f",
   "metadata": {},
   "source": [
    "## Comparaci√≥n cualitativa entre modelo base y modelo fine-tuneado\n",
    "\n",
    "Generamos respuestas con el modelo base y con el modelo fine tunned usando exactamente el mismo prompt. Esto permite evaluar de manera r√°pida el impacto del fine-tuning de forma cualitativa, comparando estilo, coherencia y fidelidad al comportamiento esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33da88-bbf0-4fce-807f-29e1af176759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (no fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "fine_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "fine_base_model.to(device)\n",
    "\n",
    "# Load fine-tuned model (inject LoRA adapters into the base model)\n",
    "fine_model = PeftModel.from_pretrained(fine_base_model, OUTPUT_DIR).to(device)\n",
    "\n",
    "def generate_response(prompt_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a response using the model's chat template.\n",
    "    \"\"\"\n",
    "    # 1) Chat message format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener \"\n",
    "                f\"conversaciones casuales. Respondes con la misma personalidad que {target_author} \"\n",
    "                f\"tiene en los chats de WhatsApp./no_think\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_text},\n",
    "    ]\n",
    "\n",
    "    # 2) Apply chat template and prepare tensors\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 3) Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # 4) Decode and post-process\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove any <think>...</think> blocks if present\n",
    "    decoded = re.sub(r\"<think>.*?</think>\", \"\", decoded, flags=re.DOTALL)\n",
    "\n",
    "    # Heuristic: keep only the portion after the user's prompt\n",
    "    if prompt_text in decoded:\n",
    "        response = decoded.split(prompt_text)[-1].strip()\n",
    "    else:\n",
    "        response = decoded\n",
    "\n",
    "    # Extra cleanup in case role markers leak into the final text\n",
    "    for tag in [\"system\", \"user\", \"assistant\"]:\n",
    "        response = response.replace(tag, \"\")\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def compare_models(prompt):\n",
    "    print(f\"\\nPREGUNTA: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Base model\n",
    "    try:\n",
    "        base_resp = generate_response(prompt, base_model, tokenizer)\n",
    "        print(f\"Modelo BASE:\\n{base_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Base: {e}\")\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Fine-tuned model\n",
    "    try:\n",
    "        fine_resp = generate_response(prompt, fine_model, tokenizer)\n",
    "        print(f\"BOT (Fine-Tuned):\\n{fine_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Fine-Tuned: {e}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß™ PRUEBA MANUAL\n",
    "# ============================================================\n",
    "mis_preguntas = [\n",
    "    \"Hola, c√≥mo est√°s?\",\n",
    "    \"Eu sale algo el finde?\",\n",
    "    \"Qu√© opin√°s de la programaci√≥n?\",\n",
    "    \"Me aburrooo, contame algo\",\n",
    "    \"Nos vemos m√°s tarde?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== COMPARACI√ìN DE MODELOS ===\")\n",
    "for p in mis_preguntas:\n",
    "    compare_models(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aeb5c-29f1-47d0-bab6-c37df125f2a8",
   "metadata": {},
   "source": [
    "## Comparaci√≥n autom√°tica usando ejemplos del conjunto de validaci√≥n\n",
    "\n",
    "Tomamos una muestra aleatoria del conjunto de validaci√≥n y comparamos las respuestas del modelo base vs el modelo fine-tuneado, reutilizando la misma funci√≥n de generaci√≥n. Esto permite inspeccionar r√°pidamente diferencias de estilo y coherencia sobre datos reales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b45da-d49c-47a6-aa82-9a3787c13029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Safety checks\n",
    "if \"dataset\" not in globals():\n",
    "    raise RuntimeError(\"La variable 'dataset' no existe. Ejecut√° primero la preparaci√≥n del dataset.\")\n",
    "\n",
    "if \"base_model\" not in globals() or \"fine_model\" not in globals():\n",
    "    raise RuntimeError(\"base_model y/o fine_model no est√°n cargados. Ejecut√° primero el comparador de modelos.\")\n",
    "\n",
    "if \"tokenizer\" not in globals():\n",
    "    raise RuntimeError(\"La variable 'tokenizer' no existe. Asegurate de cargar el tokenizer primero.\")\n",
    "\n",
    "if \"generate_response\" not in globals():\n",
    "    raise RuntimeError(\"La funci√≥n 'generate_response' no existe. Ejecut√° primero la celda del comparador.\")\n",
    "\n",
    "# --- Robust extraction for your serialized format (<|im_start|>user ... <|im_end|>)\n",
    "IM_USER_BLOCK_RE = re.compile(\n",
    "    r\"<\\|im_start\\|>\\s*user\\s*(.*?)\\s*<\\|im_end\\|>\",\n",
    "    flags=re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_last_user_message(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the last user message from a chat-template serialized sample.\n",
    "    This implementation matches the format shown in your dataset:\n",
    "      <|im_start|>user ... <|im_end|>\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    matches = IM_USER_BLOCK_RE.findall(text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "\n",
    "    user_msg = matches[-1].strip()\n",
    "\n",
    "    # Minimal cleanup for display / prompting\n",
    "    user_msg = user_msg.replace(\"<|msg_sep|>\", \" \")\n",
    "    user_msg = re.sub(r\"\\s+\", \" \", user_msg).strip()\n",
    "    return user_msg\n",
    "\n",
    "# --- Validation sampling\n",
    "val_split = dataset[\"test\"]\n",
    "n_samples = min(5, len(val_split))\n",
    "\n",
    "if n_samples == 0:\n",
    "    raise RuntimeError(\"El split de validaci√≥n est√° vac√≠o (dataset['test']). Revis√° el train_test_split.\")\n",
    "\n",
    "sample_idx = random.sample(range(len(val_split)), n_samples)\n",
    "\n",
    "print(\"\\n=== COMPARACI√ìN AUTOM√ÅTICA (CONJUNTO DE VALIDACI√ìN) ===\")\n",
    "print(f\"Se evaluar√°n {n_samples} ejemplos aleatorios del split 'test'.\")\n",
    "\n",
    "# --- Compare base vs fine-tuned on validation prompts\n",
    "shown = 0\n",
    "for idx in sample_idx:\n",
    "    sample_text = val_split[idx][\"text\"]\n",
    "    user_prompt = extract_last_user_message(sample_text)\n",
    "\n",
    "    if not user_prompt:\n",
    "        continue\n",
    "\n",
    "    shown += 1\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Ejemplo #{idx}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Prompt (√∫ltimo mensaje del usuario):\")\n",
    "    print(user_prompt)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Modelo BASE:\")\n",
    "    try:\n",
    "        print(generate_response(user_prompt, base_model, tokenizer))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Modelo FINE-TUNEADO:\")\n",
    "    try:\n",
    "        print(generate_response(user_prompt, fine_model, tokenizer))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Comparaci√≥n finalizada. Ejemplos mostrados: {shown}/{n_samples}.\")\n",
    "if shown < n_samples:\n",
    "    print(\"Nota: algunos ejemplos no pudieron usarse porque no se detect√≥ el bloque <|im_start|>user ... <|im_end|>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cec77-3e79-46b4-8492-e0533fca9c7e",
   "metadata": {},
   "source": [
    "## An√°lisis estad√≠stico y calidad del dataset final\n",
    "\n",
    "Este bloque analiza el dataset conversacional final utilizado para el entrenamiento supervisado del modelo.\n",
    "El objetivo es caracterizar la estructura y el contenido del conjunto de datos definitivo, evaluar su calidad ling√º√≠stica y verificar que sea adecuado para el proceso de fine-tuning.\n",
    "\n",
    "En particular, se reportan las siguientes m√©tricas:\n",
    "\n",
    "* **Samples**: cantidad total de ejemplos de entrenamiento. Cada ejemplo corresponde a una conversaci√≥n serializada en formato chat-template que el modelo debe aprender a continuar.\n",
    "\n",
    "* **Exact duplicates**: n√∫mero de ejemplos id√©nticos detectados dentro del dataset final. Un valor bajo indica que no existe sobre-representaci√≥n artificial de conversaciones repetidas.\n",
    "\n",
    "* **Chars avg**: longitud promedio de los ejemplos medida en caracteres. Proporciona una noci√≥n general del tama√±o del texto sin depender del tokenizer.\n",
    "\n",
    "* **Words avg**: cantidad promedio de palabras por ejemplo, calculada tras eliminar tokens de control y residuos del template. Sirve como medida intuitiva de la longitud del contenido ling√º√≠stico.\n",
    "\n",
    "* **Words p95**: percentil 95 de la longitud en palabras. Indica que el 95% de los ejemplos tiene una longitud menor o igual a este valor, permitiendo identificar conversaciones particularmente largas.\n",
    "\n",
    "* **Unique words**: cantidad total de palabras distintas presentes en el dataset final, utilizada como estimaci√≥n del tama√±o del vocabulario efectivo.\n",
    "\n",
    "* **Vocab richness (%)**: proporci√≥n entre palabras √∫nicas y el total de palabras. Valores relativamente bajos son esperables en chats personales y reflejan consistencia de estilo y repetici√≥n de expresiones caracter√≠sticas del hablante.\n",
    "\n",
    "* **Tokens avg**: cantidad promedio de tokens por ejemplo seg√∫n el tokenizer del modelo, lo que representa la longitud real que el modelo procesa durante el entrenamiento.\n",
    "\n",
    "* **Tokens p95**: percentil 95 de la longitud en tokens. Es una m√©trica clave para estimar el uso del contexto y reducir el riesgo de truncamiento durante el fine-tuning.\n",
    "\n",
    "Adem√°s, se incluye un an√°lisis de frecuencia l√©xica del dataset final:\n",
    "\n",
    "* Un ranking de las palabras m√°s frecuentes manteniendo muletillas y expresiones coloquiales, √∫til para observar patrones conversacionales dominantes.\n",
    "\n",
    "* Un ranking alternativo sin muletillas ni stopwords, orientado a resaltar el contenido sem√°nticamente informativo del dataset.\n",
    "\n",
    "* Una nube de palabras construida a partir del vocabulario informativo del dataset final, donde el tama√±o de cada palabra es proporcional a su frecuencia de aparici√≥n, permitiendo una visualizaci√≥n global del estilo y los temas predominantes que el modelo aprender√°.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1562b-c343-4518-bccc-843c8eee4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: word cloud visualization\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    WORDCLOUD_AVAILABLE = True\n",
    "except Exception:\n",
    "    WORDCLOUD_AVAILABLE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Safety checks\n",
    "# -----------------------------\n",
    "assert \"data\" in globals() and isinstance(data, pd.DataFrame) and \"text\" in data.columns, \\\n",
    "    \"‚ùå data (dataset final) no existe o no tiene columna 'text'\"\n",
    "\n",
    "# -----------------------------\n",
    "# Regex & token helpers\n",
    "# -----------------------------\n",
    "TAG_RE = re.compile(r\"<\\|.*?\\|>\")\n",
    "WORD_RE = re.compile(r\"[a-z√°√©√≠√≥√∫√±√º0-9']+\", flags=re.IGNORECASE)\n",
    "\n",
    "SYSTEM_BLOCK_RE = re.compile(r\"<\\|im_start\\|>system.*?<\\|im_start\\|>\", re.DOTALL | re.IGNORECASE)\n",
    "THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL | re.IGNORECASE)\n",
    "IM_TAG_RE = re.compile(r\"<\\|im_(start|end)\\|>\", re.IGNORECASE)\n",
    "MSG_SEP_RE = re.compile(r\"<\\|msg_sep\\|>\", re.IGNORECASE)\n",
    "METADATA_RE = re.compile(r\"##\\s*metadata.*?(?=<\\|im_start\\|>|$)\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "# -----------------------------\n",
    "# Dynamic stopwords\n",
    "# -----------------------------\n",
    "AUTHOR_STOP_WORDS = set()\n",
    "if \"target_author\" in globals() and isinstance(target_author, str) and target_author.strip():\n",
    "    AUTHOR_STOP_WORDS = {\n",
    "        w for w in re.findall(r\"[a-z√°√©√≠√≥√∫√±√º]+\", target_author.lower())\n",
    "        if len(w) >= 2\n",
    "    }\n",
    "\n",
    "SYSTEM_PROMPT_WORDS = set()\n",
    "if \"bot_name\" in globals() and \"target_author\" in globals():\n",
    "    sys_prompt = (\n",
    "        f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener \"\n",
    "        f\"conversaciones casuales. Respondes con la misma personalidad que {target_author} \"\n",
    "        f\"tiene en los chats de WhatsApp.\"\n",
    "    )\n",
    "    SYSTEM_PROMPT_WORDS = {\n",
    "        w for w in re.findall(r\"[a-z√°√©√≠√≥√∫√±√º]+\", sys_prompt.lower())\n",
    "        if len(w) >= 2\n",
    "    }\n",
    "\n",
    "STOP_WORDS_EXTRA = {\n",
    "    \"user\", \"assistant\", \"system\",\n",
    "    \"metadata\", \"knowledge\", \"cutoff\", \"date\", \"today\",\n",
    "    \"reasoning\", \"mode\", \"custom\", \"instructions\",\n",
    "    \"im_start\", \"im_end\", \"think\", \"msg_sep\", \"no_think\",\n",
    "    \"end\", \"start\"\n",
    "}.union(AUTHOR_STOP_WORDS).union(SYSTEM_PROMPT_WORDS)\n",
    "\n",
    "EN_TIME_WORDS = {\n",
    "    \"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\n",
    "    \"september\",\"october\",\"november\",\"december\",\n",
    "    \"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"\n",
    "}\n",
    "STOP_WORDS_EXTRA |= EN_TIME_WORDS\n",
    "\n",
    "SPANISH_STOPWORDS = {\n",
    "    \"el\",\"la\",\"los\",\"las\",\"un\",\"una\",\"unos\",\"unas\",\n",
    "    \"yo\",\"me\",\"te\",\"se\",\"lo\",\"le\",\"nos\",\"les\",\n",
    "    \"de\",\"que\",\"y\",\"o\",\"pero\",\"si\",\"no\",\"es\",\"en\",\"con\",\"por\",\"para\",\n",
    "    \"ya\",\"bien\",\"eh\",\"ah\",\"oh\",\"xd\",\"jaja\",\"jajaja\",\n",
    "    \"q\",\"k\",\"tmb\",\"tb\",\"pa\",\"pq\",\"xq\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Normalization (stats only)\n",
    "# -----------------------------\n",
    "def normalize_for_stats(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = SYSTEM_BLOCK_RE.sub(\" \", text)\n",
    "    text = METADATA_RE.sub(\" \", text)\n",
    "    text = THINK_RE.sub(\" \", text)\n",
    "    text = IM_TAG_RE.sub(\" \", text)\n",
    "    text = MSG_SEP_RE.sub(\" \", text)\n",
    "    text = TAG_RE.sub(\" \", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_words_raw(text: str):\n",
    "    words = WORD_RE.findall(normalize_for_stats(text))\n",
    "    return [w.lower() for w in words if w.lower() not in STOP_WORDS_EXTRA and len(w) >= 2]\n",
    "\n",
    "def extract_words_informative(text: str):\n",
    "    return [w for w in extract_words_raw(text) if w not in SPANISH_STOPWORDS]\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset-level statistics\n",
    "# -----------------------------\n",
    "texts = data[\"text\"].astype(str).tolist()\n",
    "texts = [t for t in texts if t.strip()]\n",
    "n = len(texts)\n",
    "\n",
    "char_lengths = [len(t) for t in texts]\n",
    "word_lengths = [len(extract_words_raw(t)) for t in texts]\n",
    "\n",
    "vocab_raw = Counter()\n",
    "vocab_info = Counter()\n",
    "for t in texts:\n",
    "    vocab_raw.update(extract_words_raw(t))\n",
    "    vocab_info.update(extract_words_informative(t))\n",
    "\n",
    "stats = {\n",
    "    \"samples\": n,\n",
    "    \"exact_duplicates\": int(data[\"text\"].duplicated().sum()),\n",
    "    \"chars_avg\": sum(char_lengths) / max(1, n),\n",
    "    \"words_avg\": sum(word_lengths) / max(1, n),\n",
    "    \"words_p95\": float(pd.Series(word_lengths).quantile(0.95)) if n else 0.0,\n",
    "    \"unique_words\": int(len(vocab_raw)),\n",
    "    \"vocab_richness_pct\": 100.0 * len(vocab_raw) / max(1, sum(vocab_raw.values())),\n",
    "}\n",
    "\n",
    "if \"tokenizer\" in globals() and tokenizer is not None:\n",
    "    try:\n",
    "        token_lengths = [len(tokenizer(t, add_special_tokens=False).input_ids) for t in texts]\n",
    "        stats[\"tokens_avg\"] = sum(token_lengths) / max(1, n)\n",
    "        stats[\"tokens_p95\"] = float(pd.Series(token_lengths).quantile(0.95))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas del dataset final (SmolLM3)\")\n",
    "display(pd.DataFrame([stats]))\n",
    "\n",
    "# -----------------------------\n",
    "# Top words: con / sin muletillas\n",
    "# -----------------------------\n",
    "TOP_K = 20\n",
    "\n",
    "top_raw = pd.DataFrame(vocab_raw.most_common(TOP_K), columns=[\"palabra\", \"frecuencia\"])\n",
    "top_info = pd.DataFrame(vocab_info.most_common(TOP_K), columns=[\"palabra\", \"frecuencia\"])\n",
    "\n",
    "max_len = max(len(top_raw), len(top_info))\n",
    "top_raw = top_raw.reindex(range(max_len))\n",
    "top_info = top_info.reindex(range(max_len))\n",
    "\n",
    "spacer = pd.DataFrame({\"\": [\"\"] * max_len})\n",
    "spacer.columns = pd.MultiIndex.from_product([[\"\"], [\"\"]])\n",
    "\n",
    "top_raw.columns = pd.MultiIndex.from_product([[\"Con muletillas\"], top_raw.columns])\n",
    "top_info.columns = pd.MultiIndex.from_product([[\"Sin muletillas\"], top_info.columns])\n",
    "\n",
    "print(f\"\\nüîù Top {TOP_K} palabras del dataset final\")\n",
    "display(pd.concat([top_raw, spacer, top_info], axis=1))\n",
    "\n",
    "# -----------------------------\n",
    "# Word cloud (informative)\n",
    "# -----------------------------\n",
    "if WORDCLOUD_AVAILABLE and vocab_info:\n",
    "    print(\"\\nüñºÔ∏è Nube de palabras (dataset final, sin muletillas)\")\n",
    "    wc = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color=\"white\",\n",
    "        max_words=120,\n",
    "        collocations=False\n",
    "    ).generate_from_frequencies(dict(vocab_info))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "elif not WORDCLOUD_AVAILABLE:\n",
    "    print(\"\\n‚ÑπÔ∏è WordCloud no disponible (pip install wordcloud)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30021a23-aaf3-452d-9903-6e48a9cd6bf5",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n cuantitativa y reporte final\n",
    "\n",
    "Calculamos m√©tricas cuantitativas sobre el conjunto de validaci√≥n para comparar el modelo base y el modelo fine-tuneado. Se reportan la cross-entropy promedio y la perplexity aproximada, junto con una visualizaci√≥n comparativa que resume el impacto del fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b14e9b-b085-4e53-be7c-3641d23d432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def evaluate_model(model, data_iterable, tokenizer, max_batches=100):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss and perplexity for a causal language model.\n",
    "    Expects an iterable with a 'text' field containing pre-formatted chat samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    print(f\"Midiendo m√©tricas en {max_batches} ejemplos...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(tqdm(data_iterable, total=max_batches)):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "\n",
    "            # Extract pre-formatted chat text\n",
    "            text = example[\"text\"]\n",
    "\n",
    "            # Tokenize on the fly\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            # Causal LM loss (labels = input_ids)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            losses.append(outputs.loss.item())\n",
    "\n",
    "    if not losses:\n",
    "        return {\"cross_entropy\": 0.0, \"perplexity\": 0.0}\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    try:\n",
    "        perplexity = math.exp(avg_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    return {\"cross_entropy\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "# --- VALIDATION DATA SELECTION ---\n",
    "# Prefer an explicit validation split if available\n",
    "if isinstance(dataset, dict) and \"test\" in dataset:\n",
    "    eval_data = dataset[\"test\"]\n",
    "else:\n",
    "    # Fallback: evaluate on a small subset of the dataset\n",
    "    eval_data = dataset.select(range(min(50, len(dataset))))\n",
    "\n",
    "# --- EVALUATION RUN ---\n",
    "\n",
    "print(\"\\nEvaluando modelo base...\")\n",
    "metrics_base = evaluate_model(base_model, eval_data, tokenizer)\n",
    "\n",
    "print(\"\\nEvaluando modelo fine-tuneado...\")\n",
    "metrics_fine = evaluate_model(fine_model, eval_data, tokenizer)\n",
    "\n",
    "\n",
    "# --- REPORTING ---\n",
    "print(\"\\nRESULTADOS DE EVALUACI√ìN\")\n",
    "print(\"-\" * 40)\n",
    "print(\n",
    "    f\"Base model   ‚Üí Cross-Entropy: {metrics_base['cross_entropy']:.3f} | \"\n",
    "    f\"Perplexity: {metrics_base['perplexity']:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Fine-tuned   ‚Üí Cross-Entropy: {metrics_fine['cross_entropy']:.3f} | \"\n",
    "    f\"Perplexity: {metrics_fine['perplexity']:.2f}\"\n",
    ")\n",
    "\n",
    "# Relative improvement\n",
    "if metrics_base[\"perplexity\"] > 0:\n",
    "    improvement = (\n",
    "        (metrics_base[\"perplexity\"] - metrics_fine[\"perplexity\"])\n",
    "        / metrics_base[\"perplexity\"]\n",
    "        * 100\n",
    "    )\n",
    "else:\n",
    "    improvement = 0.0\n",
    "\n",
    "print(f\"Mejora relativa en Perplexity: {improvement:.2f}%\")\n",
    "print(\"Nota: una menor perplexity indica una mejor modelizaci√≥n del estilo conversacional.\")\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "barplot = sns.barplot(\n",
    "    x=[\"Base (SmolLM)\", \"Fine-tuned\"],\n",
    "    y=[metrics_base[\"perplexity\"], metrics_fine[\"perplexity\"]],\n",
    "    palette=[\"#95a5a6\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "# Annotate bar values\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 9),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Comparaci√≥n de Perplexity\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
