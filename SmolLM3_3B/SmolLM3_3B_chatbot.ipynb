{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec623398-c458-4703-9398-012d7d2dba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.13/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.13/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.13/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.13/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.13/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.13/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.13/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.13/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.13/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.13/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.13/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.13/site-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.13/site-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.13/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/huggingface/trl.git\n",
      "  Cloning https://github.com/huggingface/trl.git to /tmp/pip-req-build-a6at7wp1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-a6at7wp1\n",
      "  Resolved https://github.com/huggingface/trl.git to commit 036ae820b3c8c57f352536015b97d5e987ee2a2c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (4.4.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (4.57.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.27.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.27.0.dev0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.27.0.dev0) (2.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers>=4.56.1->trl==0.27.0.dev0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers>=4.56.1->trl==0.27.0.dev0) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.27.0.dev0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers datasets matplotlib seaborn\n",
    "\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "%pip install -U transformers accelerate peft bitsandbytes\n",
    "\n",
    "%pip install git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb154d96-0d93-48fd-8b7e-af0009d50fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Si el entorno utiliza m√°s de una GPU, recomendamos no omitir esta linea.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449617c-2778-47b3-ba61-d2a619beb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0Ô∏è‚É£ CONSTANTES Y ARCHIVOS\n",
    "# ============================================================\n",
    "\n",
    "# TODO: pedir chat.txt ac√°\n",
    "# ---- CONSTANTES MODIFICABLES -----\n",
    "# üëâ Completar datos para personalizar tu experiencia\n",
    "chat_file = \"chat.txt\"\n",
    "target_author = \"Bot\"  # Tu nombre de usuario en Whatsapp -> exactamente como aparece en chat_file\n",
    "bot_name = \"BOT\" # Elige un nombre para el bot\n",
    "\n",
    "# ---- CONSTANTES FIJAS -----\n",
    "# En esta versi√≥n estaremos usando el siguiente modelo\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "# Usaremos este tag especial para indicar que fueron distintos mensajes,\n",
    "# pero vienen del mismo autor y fueron enviados en una ventana cercana de tiempo\n",
    "MSG_SEP = \"<|msg_sep|>\"\n",
    "# Direcci√≥n donde se estar√°n guardando los datos, como chats filtrados o el modelo.\n",
    "OUTPUT_DIR = \"./chatbot_\" + bot_name\n",
    "# Crea el directorio\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6272e-24a2-4de5-b229-f904dd785e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1Ô∏è‚É£ IMPORTACI√ìN DE LIBRER√çAS\n",
    "# ============================================================\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Imports listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8451d6b4-c471-4b56-af51-9027b8d7964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2Ô∏è‚É£ CONFIGURACI√ìN DEL DISPOSITIVO (CPU / GPU)\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9069-0174-437a-ba7f-ea83cfba6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TOKENIZER\n",
    "# ============================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5866af4-710a-4a1b-8936-29bc96621cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Procesando chat (k-turns con roles)...\n",
      "üìú Total turnos agrupados: 63969\n",
      "‚úÖ Se generaron 29498 ejemplos de entrenamiento.\n",
      "üíæ Archivo guardado en: ./chatbot_MELINA/formatted_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3Ô∏è‚É£ PROCESAMIENTO DEL CHAT WHATSAPP (k-turns con roles)\n",
    "# ============================================================\n",
    "\n",
    "irrelevantData = {\n",
    "    # espa√±ol\n",
    "    'eliminaste este mensaje',\n",
    "    'se elimin√≥ este mensaje',\n",
    "    '<multimedia omitido>',\n",
    "    'multimedia omitido',\n",
    "    'los mensajes y las llamadas est√°n cifrados de extremo a extremo',\n",
    "    # ingl√©s\n",
    "    'you deleted this message',\n",
    "    'this message was deleted',\n",
    "    '<media omitted>',\n",
    "    'media omitted',\n",
    "    'messages and calls are end-to-end encrypted',\n",
    "}\n",
    "\n",
    "def containsIrrelevantData(message: str) -> bool:\n",
    "    \"\"\"\n",
    "    Asume que `message` ya est√° en min√∫sculas.\n",
    "    Devuelve True si el mensaje contiene alg√∫n patr√≥n de basura de WhatsApp.\n",
    "    \"\"\"\n",
    "    msg = message.lower()\n",
    "    return any(irr in msg for irr in irrelevantData)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza ligera del texto: remueve s√≠mbolos raros, normaliza espacios.\n",
    "    No decide si el mensaje es basura: eso lo hace `containsIrrelevantData`.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º0-9,.;:¬°!¬ø?\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def parse_datetime(line: str):\n",
    "    \"\"\"Extrae la fecha y hora del mensaje de WhatsApp, si existe.\"\"\"\n",
    "    match = re.match(r\"(\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+)\\s-\", line)\n",
    "    if match:\n",
    "        for fmt in (\"%d/%m/%y %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
    "            try:\n",
    "                return datetime.strptime(match.group(1).replace(\",\", \"\"), fmt)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def group_consecutive_messages(messages):\n",
    "    \"\"\"\n",
    "    Une mensajes consecutivos del mismo autor en uno solo (si est√°n cerca en el tiempo).\n",
    "    Esto define los 'turnos' de conversaci√≥n.\n",
    "\n",
    "    Cuando se agrupan varios mensajes del mismo autor,\n",
    "    se insertan separadores expl√≠citos MSG_SEP entre ellos:\n",
    "      [{\"role\": \"system\", \"content\": prompt}, ..., {\"role\": \"user\", \"content\": prompt}]\n",
    "    \"\"\"\n",
    "    grouped = []\n",
    "    for author, msg, ts in messages:\n",
    "        if (\n",
    "            grouped\n",
    "            and grouped[-1][0] == author\n",
    "            and ts and grouped[-1][2]\n",
    "            and (ts - grouped[-1][2]) < timedelta(hours=1)\n",
    "        ):\n",
    "            # mismo autor y cerca en el tiempo ‚Üí mismo turno, con separador\n",
    "            prev_msg = grouped[-1][1]\n",
    "            new_msg = prev_msg + f\" {MSG_SEP} \" + msg\n",
    "            grouped[-1] = (author, new_msg, ts)\n",
    "        else:\n",
    "            grouped.append((author, msg, ts))\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def process_whatsapp_chat_with_roles(\n",
    "    filepath,\n",
    "    k_history=4,\n",
    "    time_gap=timedelta(hours=3),\n",
    "):\n",
    "    \"\"\"\n",
    "    Construye pares <PROMPT, RESPONSE> con:\n",
    "\n",
    "      ‚Ä¢ Turnos = mensajes consecutivos del mismo autor (agrupados)\n",
    "      ‚Ä¢ k_history turnos de contexto (tuyos + del otro), en orden cronol√≥gico\n",
    "      ‚Ä¢ Roles expl√≠citos: [bot_name] / [OTRO] en el prompt\n",
    "      ‚Ä¢ Dentro de cada turno, mensajes separados por MSG_SEP (<|msg_sep|>)\n",
    "      ‚Ä¢ RESPONSE = solo el turno de `target_author` (tu respuesta)\n",
    "\n",
    "    Formato final:\n",
    "\n",
    "      PROMPT   = \"<|talk|><|ax1|> [OTRO] ... <|msg_sep|> ... [bot_name] ... <|ax2|>\"\n",
    "      RESPONSE = \" tu_respuesta <|endoftext|>\"\n",
    "    \"\"\"\n",
    "    print(\"üßπ Procesando chat (k-turns con roles)...\")\n",
    "\n",
    "    # --- Leer y parsear l√≠neas crudas ---\n",
    "    messages = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            ts = parse_datetime(line)\n",
    "            match = re.match(r'\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+\\s-\\s([^:]+):\\s(.+)', line)\n",
    "            if match:\n",
    "                author = match.group(1).strip()\n",
    "                raw_msg = match.group(2)\n",
    "                msg = clean_text(raw_msg)\n",
    "                # Filtramos mensajes vac√≠os o basura de WhatsApp\n",
    "                if msg and not containsIrrelevantData(msg):\n",
    "                    messages.append((author, msg, ts))\n",
    "\n",
    "    if not messages:\n",
    "        print(\"‚ö†Ô∏è No se encontraron mensajes v√°lidos.\")\n",
    "        return [], []\n",
    "\n",
    "    # --- Agrupar consecutivos del mismo autor (turnos) ---\n",
    "    messages = group_consecutive_messages(messages)\n",
    "    print(f\"üìú Total turnos agrupados: {len(messages)}\")\n",
    "\n",
    "    formatted_data = []\n",
    "\n",
    "    # --- Recorrer turnos y construir ejemplos ---\n",
    "    for i in range(1, len(messages)):\n",
    "        author_i, msg_i, ts_i = messages[i]\n",
    "\n",
    "        # Solo nos interesa cuando VOS respond√©s\n",
    "        if author_i != target_author:\n",
    "            continue\n",
    "\n",
    "        # üëâ En caso de querer cambiar el prompt prueba modificar \"content\"\n",
    "        conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener conversaciones casuales. Respondes con la misma personalidad que {target_author} tiene en los chats de WhatsApp./no_think\"}\n",
    "        ]\n",
    "\n",
    "        # Buscar contexto hacia atr√°s (k_history)\n",
    "        temp_history = []\n",
    "        last_ts = ts_i\n",
    "\n",
    "        for j in range(i - 1, -1, -1):\n",
    "            a_j, m_j, ts_j = messages[j]\n",
    "            if ts_j and last_ts and (last_ts - ts_j) > time_gap:\n",
    "                break\n",
    "\n",
    "            # Asignamos ROLES est√°ndar: 'user' (el otro) o 'assistant' (t√∫ en Whatsapp)\n",
    "            role = \"assistant\" if a_j == target_author else \"user\"\n",
    "            temp_history.insert(0, {\"role\": role, \"content\": m_j})\n",
    "\n",
    "            last_ts = ts_j if ts_j is not None else last_ts\n",
    "            if len(temp_history) >= k_history:\n",
    "                break\n",
    "\n",
    "        if not temp_history:\n",
    "            continue\n",
    "\n",
    "        # Agregamos la historia al chat\n",
    "        conversation_history.extend(temp_history)\n",
    "\n",
    "        # Agregamos TU respuesta actual (lo que el modelo debe predecir)\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": msg_i})\n",
    "\n",
    "        # --- LA MAGIA: apply_chat_template ---\n",
    "        # Esto convierte la lista en el string exacto que el modelo necesita.\n",
    "        # Por ejemplo: \"<|im_start|>user\\nHola<|msg_sep|>C√≥mo estas?<|im_end|>\\n<|im_start|>assistant...\"\n",
    "        final_text = tokenizer.apply_chat_template(conversation_history, tokenize=False, enable_thinking=False) # TODO: ver con enable_thinking = True\n",
    "\n",
    "        formatted_data.append(final_text)\n",
    "\n",
    "    print(f\"‚úÖ Se generaron {len(formatted_data)} ejemplos de entrenamiento.\")\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "# üíæ Datos filtrados y formateados guardados\n",
    "formatted_data = process_whatsapp_chat_with_roles(chat_file)\n",
    "df = pd.DataFrame({\"text\": formatted_data})\n",
    "df.to_json(f\"{OUTPUT_DIR}/formatted_data.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "print(f\"üíæ Archivo guardado en: {OUTPUT_DIR}/formatted_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa2ad9-122c-4366-a492-36edbcea7537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dataset listo para TRL: 27964 conversaciones.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4Ô∏è‚É£ PREPARACI√ìN DE DATOS\n",
    "# ============================================================\n",
    "data = pd.read_json(f\"{OUTPUT_DIR}/formatted_data.jsonl\", lines=True)\n",
    "\n",
    "# Limpieza b√°sica: Eliminar ejemplos vac√≠os o muy cortos\n",
    "data = data[data[\"text\"].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "# Eliminaci√≥n de URLs (Tu l√≥gica original, adaptada)\n",
    "data = data[~data[\"text\"].str.contains(r\"http|www|\\.com\", regex=True)]\n",
    "\n",
    "print(f\"üìÅ Dataset listo para TRL: {len(data)} conversaciones.\")\n",
    "\n",
    "# Formato HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Dividir Train/Test\n",
    "dataset = dataset.train_test_split(test_size=0.1) # 10% para validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60446b-ad4f-4e26-bf6a-1e642c35c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo HuggingFaceTB/SmolLM3-3B en 4-bits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df4cbfe832f44c8ad66fbfcdcdbd771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßê Memoria del modelo: torch.bfloat16\n",
      "‚öñÔ∏è Footprint de memoria: 1.80 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5Ô∏è‚É£ CARGA DEL MODELO (Con LoRA para eficiencia y m√©tricas)\n",
    "# ============================================================\n",
    "\n",
    "# Configuraci√≥n de Cuantizaci√≥n (Para eficiencia)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "print(f\"‚è≥ Cargando modelo {MODEL_ID}...\")\n",
    "\n",
    "# Cargar Modelo Base\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Tokenizer fue cargado previamente\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configuraci√≥n LoRA (Low-Rank Adaptation)\n",
    "# Esto permite entrenar solo un % peque√±o de par√°metros (m√°s r√°pido y ligero)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,       \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    loss = eval_pred.loss \n",
    "    try:\n",
    "        perp = math.exp(loss)\n",
    "    except OverflowError:\n",
    "        perp = float(\"inf\")\n",
    "    return {\"loss\": loss, \"perplexity\": perp}\n",
    "\n",
    "print(f\"‚úÖ Modelo {MODEL_ID} cargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ced775-8ba0-41a0-b9f8-bfa9d0a6eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.13/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94509018e4074990bd73795460869dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/25167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c9e6ad6eb84af793cd3f558707832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/25167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05721310b6e14c1e888d8489469bd9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/25167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e898959bc034a8cbae8e7aaf50587a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9611a36086144adeadbab4e5d2f141ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be9789b1cb4c4ab718f9856f3b2f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ü§ñ CONFIGURACI√ìN DEL SFT TRAINER\n",
    "# ============================================================\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,      \n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,                \n",
    "    fp16=False,\n",
    "    bf16=True,                         # Dada la gr√°fica utilizada esto era conveniente\n",
    "    eval_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],   \n",
    "    peft_config=peft_config,  \n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Configuraci√≥n cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a7891-4346-4a99-b32a-0871f91866be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando entrenamiento con SFTTrainer... \n",
      " Esto podr√° tomar un tiempo ü•±‚òï‚è∞ \n",
      " El modelo estar√° üíæ guardado en ./chatbot_MELINA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9438' max='9438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9438/9438 2:59:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.249900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>1.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>1.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>1.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>1.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>1.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>1.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>1.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>1.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>1.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>1.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>1.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>1.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.206800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>1.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>1.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>1.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>1.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>1.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>1.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>1.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>1.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>1.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>1.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>1.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>1.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>1.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>1.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>1.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>1.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>1.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>1.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>1.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.071400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>1.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>1.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>1.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>1.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>1.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>1.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>1.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>1.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>1.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>1.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>1.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>1.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>1.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>1.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>1.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>1.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>1.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>1.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>1.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>1.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>1.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>1.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>1.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>1.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>1.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>1.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>1.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>1.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>1.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>1.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>1.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>1.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>1.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>1.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>1.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>1.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>1.213400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>1.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>1.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>1.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>1.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>1.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>1.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>1.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>1.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>1.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>1.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>1.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>1.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>1.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>1.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>1.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>1.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>1.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>1.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>1.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>1.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>1.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>1.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>1.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>1.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>1.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>1.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>1.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>1.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>1.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>1.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>1.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>1.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>1.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>1.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>1.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>1.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>1.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>1.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>1.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>1.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>1.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>1.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>1.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>1.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>1.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>1.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>0.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>1.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>1.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>1.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>1.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>1.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>1.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>1.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>1.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>1.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>1.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>1.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>1.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>1.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>1.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>1.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>1.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>1.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>1.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>1.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>1.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>1.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>1.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>1.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>1.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>1.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>1.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>1.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>1.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>1.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>1.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>1.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>1.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>1.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>1.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>1.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>1.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>1.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>1.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>1.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>1.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>1.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>1.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>1.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>1.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>1.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>1.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>1.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>1.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>1.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5970</td>\n",
       "      <td>1.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>1.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5990</td>\n",
       "      <td>1.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6010</td>\n",
       "      <td>1.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>1.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6030</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>1.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6070</td>\n",
       "      <td>1.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>1.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6130</td>\n",
       "      <td>1.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>1.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>1.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6170</td>\n",
       "      <td>1.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>1.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6190</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.087700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>1.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6230</td>\n",
       "      <td>0.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>1.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6270</td>\n",
       "      <td>1.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6290</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6310</td>\n",
       "      <td>1.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>1.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>1.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>1.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>1.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>1.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6410</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6430</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6470</td>\n",
       "      <td>1.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>1.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6490</td>\n",
       "      <td>1.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6510</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>1.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6530</td>\n",
       "      <td>1.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>1.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6570</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6590</td>\n",
       "      <td>1.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6610</td>\n",
       "      <td>1.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6630</td>\n",
       "      <td>1.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>1.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>1.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6690</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6710</td>\n",
       "      <td>1.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>1.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6730</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>1.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6760</td>\n",
       "      <td>1.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6770</td>\n",
       "      <td>1.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6780</td>\n",
       "      <td>1.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6790</td>\n",
       "      <td>1.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6810</td>\n",
       "      <td>1.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>1.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6830</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6840</td>\n",
       "      <td>1.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>1.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6870</td>\n",
       "      <td>1.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>0.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6890</td>\n",
       "      <td>1.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6910</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6920</td>\n",
       "      <td>1.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6930</td>\n",
       "      <td>0.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6940</td>\n",
       "      <td>1.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6960</td>\n",
       "      <td>1.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6970</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6980</td>\n",
       "      <td>1.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6990</td>\n",
       "      <td>1.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7010</td>\n",
       "      <td>0.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>1.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7030</td>\n",
       "      <td>1.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7040</td>\n",
       "      <td>1.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>1.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7070</td>\n",
       "      <td>1.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7080</td>\n",
       "      <td>1.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7090</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7110</td>\n",
       "      <td>1.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7120</td>\n",
       "      <td>1.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7130</td>\n",
       "      <td>0.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>0.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7160</td>\n",
       "      <td>1.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7170</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7180</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7190</td>\n",
       "      <td>1.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7210</td>\n",
       "      <td>1.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7220</td>\n",
       "      <td>1.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7230</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7240</td>\n",
       "      <td>1.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7260</td>\n",
       "      <td>1.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7270</td>\n",
       "      <td>1.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7280</td>\n",
       "      <td>1.101900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7290</td>\n",
       "      <td>1.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7310</td>\n",
       "      <td>1.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7320</td>\n",
       "      <td>1.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7330</td>\n",
       "      <td>1.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7340</td>\n",
       "      <td>1.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>1.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7370</td>\n",
       "      <td>1.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7380</td>\n",
       "      <td>1.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7390</td>\n",
       "      <td>1.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7410</td>\n",
       "      <td>1.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7420</td>\n",
       "      <td>1.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7430</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>1.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>1.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7480</td>\n",
       "      <td>1.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7490</td>\n",
       "      <td>1.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7510</td>\n",
       "      <td>1.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7520</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7530</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7540</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7560</td>\n",
       "      <td>1.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7570</td>\n",
       "      <td>1.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7580</td>\n",
       "      <td>0.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7590</td>\n",
       "      <td>1.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7610</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7630</td>\n",
       "      <td>1.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7640</td>\n",
       "      <td>1.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7660</td>\n",
       "      <td>1.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7670</td>\n",
       "      <td>1.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7690</td>\n",
       "      <td>1.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7710</td>\n",
       "      <td>1.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7720</td>\n",
       "      <td>1.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7730</td>\n",
       "      <td>1.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7740</td>\n",
       "      <td>1.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7760</td>\n",
       "      <td>1.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7770</td>\n",
       "      <td>1.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7780</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7790</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7810</td>\n",
       "      <td>1.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7830</td>\n",
       "      <td>1.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7840</td>\n",
       "      <td>1.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7860</td>\n",
       "      <td>1.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7870</td>\n",
       "      <td>1.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7880</td>\n",
       "      <td>1.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7890</td>\n",
       "      <td>0.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7910</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7920</td>\n",
       "      <td>0.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7930</td>\n",
       "      <td>1.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7940</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7960</td>\n",
       "      <td>1.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7970</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7980</td>\n",
       "      <td>1.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7990</td>\n",
       "      <td>1.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8010</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8020</td>\n",
       "      <td>1.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8030</td>\n",
       "      <td>1.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8040</td>\n",
       "      <td>1.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8060</td>\n",
       "      <td>1.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8070</td>\n",
       "      <td>1.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8080</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8090</td>\n",
       "      <td>1.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8110</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8120</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8130</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8140</td>\n",
       "      <td>1.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8160</td>\n",
       "      <td>1.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8170</td>\n",
       "      <td>1.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8180</td>\n",
       "      <td>1.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8190</td>\n",
       "      <td>1.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8210</td>\n",
       "      <td>1.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8220</td>\n",
       "      <td>0.993900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8230</td>\n",
       "      <td>1.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8240</td>\n",
       "      <td>1.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8260</td>\n",
       "      <td>1.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8270</td>\n",
       "      <td>1.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8280</td>\n",
       "      <td>1.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8290</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8310</td>\n",
       "      <td>1.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8320</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8330</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8340</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8360</td>\n",
       "      <td>1.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8370</td>\n",
       "      <td>1.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8380</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8390</td>\n",
       "      <td>0.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8410</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8420</td>\n",
       "      <td>1.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8430</td>\n",
       "      <td>1.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8440</td>\n",
       "      <td>1.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8460</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8470</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8480</td>\n",
       "      <td>1.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8490</td>\n",
       "      <td>1.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8510</td>\n",
       "      <td>1.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8520</td>\n",
       "      <td>1.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8530</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>1.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8560</td>\n",
       "      <td>1.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8570</td>\n",
       "      <td>1.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8580</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8590</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8610</td>\n",
       "      <td>1.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8620</td>\n",
       "      <td>1.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8630</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8640</td>\n",
       "      <td>1.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8660</td>\n",
       "      <td>1.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8670</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8680</td>\n",
       "      <td>1.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8690</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8710</td>\n",
       "      <td>1.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8720</td>\n",
       "      <td>1.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8730</td>\n",
       "      <td>1.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8740</td>\n",
       "      <td>1.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8760</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8770</td>\n",
       "      <td>1.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8780</td>\n",
       "      <td>1.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8790</td>\n",
       "      <td>1.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8810</td>\n",
       "      <td>1.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8820</td>\n",
       "      <td>1.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8830</td>\n",
       "      <td>1.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8840</td>\n",
       "      <td>1.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8860</td>\n",
       "      <td>1.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8870</td>\n",
       "      <td>1.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8880</td>\n",
       "      <td>1.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8890</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8910</td>\n",
       "      <td>0.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8920</td>\n",
       "      <td>1.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8930</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8940</td>\n",
       "      <td>1.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8960</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8970</td>\n",
       "      <td>1.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8980</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8990</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9010</td>\n",
       "      <td>1.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9020</td>\n",
       "      <td>1.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9030</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9040</td>\n",
       "      <td>1.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9060</td>\n",
       "      <td>1.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9070</td>\n",
       "      <td>1.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9080</td>\n",
       "      <td>1.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9090</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9110</td>\n",
       "      <td>1.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9120</td>\n",
       "      <td>1.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9130</td>\n",
       "      <td>1.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9140</td>\n",
       "      <td>1.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9160</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9170</td>\n",
       "      <td>1.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9180</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9190</td>\n",
       "      <td>1.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9210</td>\n",
       "      <td>1.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9220</td>\n",
       "      <td>1.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9230</td>\n",
       "      <td>1.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9260</td>\n",
       "      <td>1.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9270</td>\n",
       "      <td>1.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9280</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9290</td>\n",
       "      <td>1.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9310</td>\n",
       "      <td>1.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9320</td>\n",
       "      <td>1.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9330</td>\n",
       "      <td>1.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9340</td>\n",
       "      <td>1.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9360</td>\n",
       "      <td>1.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9370</td>\n",
       "      <td>1.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9380</td>\n",
       "      <td>1.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9390</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9410</td>\n",
       "      <td>1.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9420</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9430</td>\n",
       "      <td>1.068000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./chatbot_MELINA/tokenizer_config.json',\n",
       " './chatbot_MELINA/special_tokens_map.json',\n",
       " './chatbot_MELINA/chat_template.jinja',\n",
       " './chatbot_MELINA/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6Ô∏è‚É£ ENTRENAMIENTO Y GUARDADO\n",
    "# ============================================================\n",
    "print(f\"üöÄ Iniciando entrenamiento... \\n Esto podr√° tomar un tiempo ü•±‚òï‚è∞\")\n",
    "try:\n",
    "    trainer.train()\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"üíæ Modelo guardado en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b410d",
   "metadata": {},
   "source": [
    "## Guardar y Descomprimir el Modelo\n",
    "Pasos opcionales pero recomendados.    \n",
    "1. **Guarda** tu modelo (carpeta comprimida) en tu computadora si est√°s desde Collab u otro entorno virtual con la primer celda\n",
    "2. Si ya entrenaste el modelo y cuentas con tu carpeta comprimida, subela a Collab y **descomprime** con la segunda celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7895b1-d24f-4f93-84d1-9d5047780082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: chatbot_MELINA/ (stored 0%)\n",
      "  adding: chatbot_MELINA/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: chatbot_MELINA/.ipynb_checkpoints/formatted_data-checkpoint.jsonl (deflated 92%)\n",
      "  adding: chatbot_MELINA/formatted_data.jsonl (deflated 92%)\n",
      "  adding: chatbot_MELINA/README.md (deflated 43%)\n",
      "  adding: chatbot_MELINA/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_MELINA/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_MELINA/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_MELINA/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_MELINA/training_args.bin (deflated 53%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/ (stored 0%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/README.md (deflated 65%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/adapter_config.json (deflated 57%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/training_args.bin (deflated 53%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/optimizer.pt (deflated 24%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/scheduler.pt (deflated 62%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/rng_state.pth (deflated 26%)\n",
      "  adding: chatbot_MELINA/checkpoint-9400/trainer_state.json (deflated 81%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/ (stored 0%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/README.md (deflated 65%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/adapter_config.json (deflated 57%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/training_args.bin (deflated 53%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/optimizer.pt (deflated 24%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/scheduler.pt (deflated 62%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/rng_state.pth (deflated 26%)\n",
      "  adding: chatbot_MELINA/checkpoint-9438/trainer_state.json (deflated 81%)\n",
      "  adding: chatbot_MELINA/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_MELINA/adapter_config.json (deflated 57%)\n",
      "‚ö† Luego de que la carpeta se haya comprimido, no te olvides de descargarla\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Comprime el archivo\n",
    "# ============================================================\n",
    "file_zip = bot_name + \".zip\"\n",
    "\n",
    "!zip -r {file_zip} {OUTPUT_DIR}\n",
    "\n",
    "print(\"‚ö† Luego de que la carpeta se haya comprimido, no te olvides de descargarla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7c6a1-83d8-47b2-8bc5-3794a87c682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  MELINA.zip\n",
      "replace chatbot_MELINA/.ipynb_checkpoints/formatted_data-checkpoint.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "‚úÖ Carpeta descomprimida\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  2. Descomprime el archivo\n",
    "# ============================================================\n",
    "file_zip = bot_name + \".zip\"\n",
    "\n",
    "# Para no volver a correr todo el c√≥digo, sube tu carpeta .zip generada en la celda anterior y ejecuta lo siguiente\n",
    "!unzip {file_zip}\n",
    "print(\"‚úÖ Carpeta descomprimida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33da88-bbf0-4fce-807f-29e1af176759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcba3fce64346689ade702015ae79ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492589334d704a9dac0901537326f0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACI√ìN DE MODELOS ===\n",
      "\n",
      "üó£Ô∏è PREGUNTA: Hola, c√≥mo est√°s?\n",
      "--------------------------------------------------\n",
      "üîπ Modelo BASE (Aburrido):\n",
      "¬°Hola! Me siento genial, gracias por preguntar. ¬øY t√∫? ¬øC√≥mo est√°s hoy? Si necesitas algo o quieres charlar, estoy aqu√≠ para ayudarte.\n",
      "--------------------\n",
      "üî∏ TU CLON (Fine-Tuned):\n",
      "bien\n",
      "==================================================\n",
      "\n",
      "üó£Ô∏è PREGUNTA: Eu sale algo el finde?\n",
      "--------------------------------------------------\n",
      "üîπ Modelo BASE (Aburrido):\n",
      "Claro, me encantar√≠a salir contigo este fin de semana! ¬øQu√© tal si nos vemos a las 7 PM en tu lugar favorito? Podr√≠amos hablar sobre todo lo que quieras y hacer una noche inolvidable juntos. ¬°Estoy emocionada por verte!\n",
      "--------------------\n",
      "üî∏ TU CLON (Fine-Tuned):\n",
      "si <|msg_sep|> mi mam√° dice q si le digo a mis abuelos no les molesta... puedo ir al centro <|msg_sep|> pero necesito q me lleven a casa despu√©s <|msg_sep|> entonces voy a preguntarles cuando llegue\n",
      "==================================================\n",
      "\n",
      "üó£Ô∏è PREGUNTA: Qu√© opin√°s de la programaci√≥n?\n",
      "--------------------------------------------------\n",
      "üîπ Modelo BASE (Aburrido):\n",
      "¬°Hola! Me encanta la programaci√≥n. Es como resolver puzzles o construir algo desde cero. Aunque puede ser compleja a veces, siempre hay una sensaci√≥n de logro cuando ves c√≥mo tu c√≥digo hace funcionar lo que no pod√≠as imaginar antes. Adem√°s, es muy vers√°til y te permite hacer muchas cosas interesantes. ¬øY t√∫, qu√© opini√≥n tienes sobre la programaci√≥n?\n",
      "--------------------\n",
      "üî∏ TU CLON (Fine-Tuned):\n",
      "mmmh no s√© <|msg_sep|> aparte me cuesta mucho el l√≥gico y eso ya es casi todo para m√≠\n",
      "==================================================\n",
      "\n",
      "üó£Ô∏è PREGUNTA: Me aburrooo, contame algo\n",
      "--------------------------------------------------\n",
      "üîπ Modelo BASE (Aburrido):\n",
      "¬°Claro! Si te aburres, podemos hacer algo divertido y relajante juntos. ¬øQu√© tal si hacemos un juego mental? Por ejemplo, intentamos resolver acertijos o juegos de palabras. O, si prefieres algo m√°s pasivo, podr√≠amos escuchar m√∫sica, ver una pel√≠cula o simplemente charlar sobre nuestras aficiones favoritas. Tambi√©n hay muchas apps y sitios web donde puedes encontrar actividades creativas para pasar el tiempo, como dibuj\n",
      "--------------------\n",
      "üî∏ TU CLON (Fine-Tuned):\n",
      "ahh bueno <|msg_sep|> te acord√°s el juego mario kart? <|msg_sep|> como cuando era chico <|msg_sep|> y q nos hac√≠a re√≠r xd <|msg_sep|> tipo el asistente del mago <|msg_sep|> o a veces ese otro tipo q le dec√≠as si o no <|msg_sep|> pero nada mas me viene ahora a la mente <|msg_sep|> aparte yo estaba muy timida entonces siempre\n",
      "==================================================\n",
      "\n",
      "üó£Ô∏è PREGUNTA: Nos vemos m√°s tarde?\n",
      "--------------------------------------------------\n",
      "üîπ Modelo BASE (Aburrido):\n",
      "¬°Claro! H√°blame m√°s tarde y cu√©ntame qu√© te ha parecido mi respuesta hasta ahora. Estoy aqu√≠ para ayudarte siempre que lo necesites. ¬°Hasta luego!\n",
      "--------------------\n",
      "üî∏ TU CLON (Fine-Tuned):\n",
      "sip <|msg_sep|> pero no s√© si me pod√©s llevar a casa xq tengo q hacer unos tr√°mites y luego voy al centro para comprar cosas <|msg_sep|> as√≠ q despu√©s de eso te digo cuando te busco\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ü§ù COMPARADOR DE RESPUESTAS ENTRE MODELOS\n",
    "# ============================================================\n",
    "\n",
    "# Carga de modelo base (sin fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "fine_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "fine_base_model.to(device)\n",
    "\n",
    "# Carga de modelo fine-tuned (inyectando LoRA)\n",
    "# Corrected line: Load adapters onto the base model\n",
    "fine_model = PeftModel.from_pretrained(fine_base_model, OUTPUT_DIR).to(device)\n",
    "\n",
    "def generate_response(prompt_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Genera respuesta usando Chat Templates modernos.\n",
    "    \"\"\"\n",
    "    # 1. Formato Chat (Standard)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener conversaciones casuales. Respondes con la misma personalidad que {target_author} tiene en los chats de WhatsApp./no_think\"}, # Opcional\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "\n",
    "    # 2. Aplicar plantilla (agrega <|im_start|>user... <|im_start|>assistant)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 3. Generar\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,      # Longitud de la respuesta\n",
    "        do_sample=True,\n",
    "        temperature=0.6,         # Recomendado por documentaci√≥n de SmolLM3-3B\n",
    "        top_p=0.95,              # Recomendado por documentaci√≥n de SmolLM3-3B\n",
    "        repetition_penalty=1.1,  # Evita bucles\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # 4. Decodificar y Limpiar\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # A. Elimina el bloque <think>...</think>\n",
    "    decoded = re.sub(r\"<think>.*?</think>\", \"\", decoded, flags=re.DOTALL)\n",
    "\n",
    "    # Buscamos d√≥nde termina el prompt del usuario en el texto decodificado.\n",
    "    if prompt_text in decoded:\n",
    "        response = decoded.split(prompt_text)[-1].strip()\n",
    "    else:\n",
    "        response = decoded\n",
    "\n",
    "    # Limpieza extra de \"system\" si se col√≥\n",
    "    for tag in [\"system\", \"user\", \"assistant\"]:\n",
    "        response = response.replace(tag, \"\")\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def compare_models(prompt):\n",
    "    print(f\"\\nüó£Ô∏è PREGUNTA: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Generar con Base\n",
    "    try:\n",
    "        base_resp = generate_response(prompt, base_model, tokenizer)\n",
    "        print(f\"üîπ Modelo BASE (Aburrido):\\n{base_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Base: {e}\")\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Generar con Fine-Tuned\n",
    "    try:\n",
    "        fine_resp = generate_response(prompt, fine_model, tokenizer)\n",
    "        print(f\"üî∏ TU CLON (Fine-Tuned):\\n{fine_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Fine-Tuned: {e}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  PRUEBA MANUAL\n",
    "# ============================================================\n",
    "mis_preguntas = [\n",
    "    \"Hola, c√≥mo est√°s?\",\n",
    "    \"Eu sale algo el finde?\",\n",
    "    \"Qu√© opin√°s de la programaci√≥n?\",\n",
    "    \"Me aburrooo, contame algo\",\n",
    "    \"Nos vemos m√°s tarde?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== COMPARACI√ìN DE MODELOS ===\")\n",
    "for p in mis_preguntas:\n",
    "    compare_models(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b14e9b-b085-4e53-be7c-3641d23d432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Evaluando Modelo BASE (Neutro)...\n",
      "üìâ Midiendo m√©tricas en 100 ejemplos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∏ Evaluando FINE-TUNED (Tu Clon)...\n",
      "üìâ Midiendo m√©tricas en 100 ejemplos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.00it/s]\n",
      "/tmp/ipykernel_9918/3046275131.py:86: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  barplot = sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RESULTADOS DE EVALUACI√ìN:\n",
      "----------------------------------------\n",
      "Base model   ‚Üí Cross-Entropy: 3.441 | Perplexity: 31.21\n",
      "Fine-tuned   ‚Üí Cross-Entropy: 1.057 | Perplexity: 2.88\n",
      "üìâ Mejora relativa en Perplexity: 90.78%\n",
      "(Nota: Menor Perplexity indica que el modelo predice mejor tu estilo)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7Ô∏è‚É£ EVALUACI√ìN Y REPORTE FINAL\n",
    "# ============================================================\n",
    "def evaluate_model(model, data_iterable, tokenizer, max_batches=100):\n",
    "    \"\"\"\n",
    "    Calcula Loss y Perplexity.\n",
    "    Acepta un dataset con columna 'text' (formato chat pre-procesado).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    print(f\"üìâ Midiendo m√©tricas en {max_batches} ejemplos...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(tqdm(data_iterable, total=max_batches)):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "\n",
    "            # 1. Obtenemos el texto (ya formateado con <|im_start|>...)\n",
    "            # Si tu dataset no tiene columna 'text', ajusta esta clave.\n",
    "            text = example[\"text\"]\n",
    "\n",
    "            # 2. Tokenizamos al vuelo\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            # 3. Calcular Loss\n",
    "            # En modelos CausalLM, si pasas labels=input_ids, calcula el loss autom√°ticamente\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            losses.append(outputs.loss.item())\n",
    "\n",
    "    if not losses:\n",
    "        return {\"cross_entropy\": 0, \"perplexity\": 0}\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    try:\n",
    "        perplexity = math.exp(avg_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    return {\"cross_entropy\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "# --- PREPARACI√ìN DE DATOS DE VALIDACI√ìN ---\n",
    "# Si dividiste el dataset antes (dataset['test']), √∫salo.\n",
    "# Si no, tomamos un pedacito del train para probar (no es ideal, pero sirve para verificar)\n",
    "if isinstance(dataset, dict) and \"test\" in dataset:\n",
    "    eval_data = dataset[\"test\"]\n",
    "else:\n",
    "    # Si dataset es un objeto √∫nico, tomamos los primeros 50 para validar\n",
    "    eval_data = dataset.select(range(50))\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "\n",
    "# 1. Evaluar MODELO BASE (Usando el truco de apagar LoRA)\n",
    "print(\"\\nüîπ Evaluando Modelo BASE (Neutro)...\")\n",
    "metrics_base = evaluate_model(base_model, eval_data, tokenizer)\n",
    "\n",
    "# 2. Evaluar TU CLON (LoRA activo)\n",
    "print(\"\\nüî∏ Evaluando FINE-TUNED (Tu Clon)...\")\n",
    "# Aqu√≠ el adaptador est√° activo por defecto\n",
    "metrics_fine = evaluate_model(fine_model, eval_data, tokenizer)\n",
    "\n",
    "\n",
    "# --- REPORTE Y GR√ÅFICOS ---\n",
    "print(\"\\nüìä RESULTADOS DE EVALUACI√ìN:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Base model   ‚Üí Cross-Entropy: {metrics_base['cross_entropy']:.3f} | Perplexity: {metrics_base['perplexity']:.2f}\")\n",
    "print(f\"Fine-tuned   ‚Üí Cross-Entropy: {metrics_fine['cross_entropy']:.3f} | Perplexity: {metrics_fine['perplexity']:.2f}\")\n",
    "\n",
    "# C√°lculo de mejora\n",
    "if metrics_base['perplexity'] > 0:\n",
    "    improvement = (metrics_base['perplexity'] - metrics_fine['perplexity']) / metrics_base['perplexity'] * 100\n",
    "else:\n",
    "    improvement = 0\n",
    "\n",
    "print(f\"üìâ Mejora relativa en Perplexity: {improvement:.2f}%\")\n",
    "print(\"(Nota: Menor Perplexity indica que el modelo predice mejor tu estilo)\")\n",
    "\n",
    "# Gr√°fico\n",
    "plt.figure(figsize=(8,5))\n",
    "barplot = sns.barplot(\n",
    "    x=[\"Base (SmolLM)\", f\"Fine-tune ({bot_name})\"],\n",
    "    y=[metrics_base['perplexity'], metrics_fine['perplexity']],\n",
    "    palette=[\"#95a5a6\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "# Agregar etiquetas de valor encima de las barras\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(f'{p.get_height():.1f}',\n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center',\n",
    "                   xytext = (0, 9),\n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.title(\"Comparaci√≥n de Perplexity (Menor es mejor)\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092595a0-57e0-4dbc-9ca4-28967de39ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
