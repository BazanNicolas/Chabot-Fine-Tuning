{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9145586-0d3b-478c-94dc-a595a61b5573",
   "metadata": {},
   "source": [
    "## Instalación de dependencias y configuración del entorno\n",
    "\n",
    "Instalamos todas las librerías necesarias para el procesamiento de datos, entrenamiento y evaluación de modelos de lenguaje. Incluye utilidades para embeddings semánticos, visualización, PyTorch con soporte CUDA y el ecosistema completo de Hugging Face, además de TRL desde GitHub para asegurar compatibilidad con configuraciones avanzadas de fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec623398-c458-4703-9398-012d7d2dba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence_transformers in /opt/conda/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.13/site-packages (0.13.2)\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.13/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in /opt/conda/lib/python3.13/site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.31.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.13/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.13/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tf-keras\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tf-keras-2.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.13/site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.13/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.13/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.13/site-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.13/site-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from transformers) (3.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.13/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/trl.git\n",
      "  Cloning https://github.com/huggingface/trl.git to /tmp/pip-req-build-tthmiedi\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-tthmiedi\n",
      "  Resolved https://github.com/huggingface/trl.git to commit a12faa5ec7ecc63b61c40074b94c82e19c13c115\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (4.4.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /opt/conda/lib/python3.13/site-packages (from trl==0.27.0.dev0) (4.57.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.13/site-packages (from accelerate>=1.4.0->trl==0.27.0.dev0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (3.20.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.13/site-packages (from datasets>=3.0.0->trl==0.27.0.dev0) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.13/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl==0.27.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.27.0.dev0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.13/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.27.0.dev0) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.27.0.dev0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.27.0.dev0) (2.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.13/site-packages (from transformers>=4.56.1->trl==0.27.0.dev0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.13/site-packages (from transformers>=4.56.1->trl==0.27.0.dev0) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.27.0.dev0) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.13/site-packages (from pandas->datasets>=3.0.0->trl==0.27.0.dev0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.27.0.dev0) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/opt/conda/lib/python3.13/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Utilities required for dataset handling, semantic embeddings, and visualization\n",
    "%pip install sentence_transformers datasets matplotlib seaborn tf-keras\n",
    "\n",
    "# Core deep learning stack (PyTorch with CUDA support on Colab)\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "# Hugging Face ecosystem for model loading, training, and quantization\n",
    "%pip install -U transformers accelerate peft bitsandbytes\n",
    "\n",
    "# TRL from GitHub to ensure compatibility with the latest SFTConfig features\n",
    "%pip install git+https://github.com/huggingface/trl.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f07b3-5d30-403c-9b93-5ee4ba385a9f",
   "metadata": {},
   "source": [
    "## Configuración del entorno de ejecución (GPU y Accelerate)\n",
    "\n",
    "Establecemos variables de entorno para controlar el uso de GPU y evitar problemas comunes al entrenar modelos en notebooks. En particular, se fuerza el uso de una única GPU y se desactiva el mixed precision de Accelerate para mejorar la estabilidad del entrenamiento en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb154d96-0d93-48fd-8b7e-af0009d50fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Restrict execution to a single GPU to avoid DataParallel-related issues in notebooks\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Disable Accelerate mixed precision for improved stability in this environment\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a601590-f22d-4ed7-9823-a98f81bbfe0f",
   "metadata": {},
   "source": [
    "## Configuración interactiva del chat\n",
    "\n",
    "Ahora subimos el archivo de chat, el autor objetivo y el nombre del bot. Esta configuración es obligatoria y debe ejecutarse antes de continuar con el notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a449617c-2778-47b3-ba61-d2a619beb6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1490ab7f4f2b4017992e2ef3c2ac119d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Configuración interactiva del chat</b>'), FileUpload(value=(), accept='.txt', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept=\".txt\",\n",
    "    multiple=False,\n",
    "    description=\"Subir chat (.txt)\"\n",
    ")\n",
    "\n",
    "author_input = widgets.Text(\n",
    "    description=\"Autor:\",\n",
    "    placeholder=\"Exactamente como aparece en el chat\",\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "bot_input = widgets.Text(\n",
    "    description=\"Bot:\",\n",
    "    placeholder=\"Nombre del bot (ej: MELINA)\",\n",
    "    layout=widgets.Layout(width=\"60%\")\n",
    ")\n",
    "\n",
    "apply_button = widgets.Button(\n",
    "    description=\"Aplicar configuración\",\n",
    "    button_style=\"primary\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_apply_clicked(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "\n",
    "        if len(uploader.value) == 0:\n",
    "            print(\"Por favor, subí un archivo .txt del chat.\")\n",
    "            return\n",
    "\n",
    "        author = author_input.value.strip()\n",
    "        bot = bot_input.value.strip()\n",
    "\n",
    "        if not author:\n",
    "            print(\"Ingresá el autor exactamente como aparece en el chat.\")\n",
    "            return\n",
    "        if not bot:\n",
    "            print(\"Ingresá un nombre para el bot.\")\n",
    "            return\n",
    "\n",
    "        upload_info = uploader.value[0]\n",
    "        content = bytes(upload_info[\"content\"]).decode(\"utf-8-sig\", errors=\"replace\")\n",
    "\n",
    "        global chat_file, target_author, bot_name, OUTPUT_DIR\n",
    "\n",
    "        chat_file = \"uploaded_chat.txt\"\n",
    "        with open(chat_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        target_author = author\n",
    "        bot_name = bot\n",
    "        OUTPUT_DIR = \"./chatbot_\" + bot_name\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        print(\"Configuración aplicada correctamente\")\n",
    "        print(\"chat_file:\", chat_file)\n",
    "        print(\"target_author:\", target_author)\n",
    "        print(\"bot_name:\", bot_name)\n",
    "        print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "apply_button.on_click(on_apply_clicked)\n",
    "\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Configuración interactiva del chat</b>\"),\n",
    "        uploader,\n",
    "        author_input,\n",
    "        bot_input,\n",
    "        apply_button,\n",
    "        out\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46aa9-24ac-440f-bb22-aa891afd14c0",
   "metadata": {},
   "source": [
    "## Constantes globales del experimento\n",
    "\n",
    "Definimos las constantes fijas del experimento y validamos que la configuración interactiva haya sido ejecutada correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a32065-7f53-43ba-8eb0-ee67ad5b669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Constantes globales cargadas correctamente\n",
      "Modelo: HuggingFaceTB/SmolLM3-3B\n",
      "Autor objetivo: Nico Bazan\n",
      "Bot: NicoBotSmol\n",
      "Directorio de salida: ./chatbot_NicoBotSmol\n"
     ]
    }
   ],
   "source": [
    "# Fixed model identifier\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "# Special token used to separate consecutive messages\n",
    "MSG_SEP = \"<|msg_sep|>\"\n",
    "\n",
    "# Safety check: interactive configuration must be executed\n",
    "required_vars = [\"chat_file\", \"target_author\", \"bot_name\", \"OUTPUT_DIR\"]\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Falta ejecutar la configuración interactiva. \"\n",
    "        f\"Variables no definidas: {missing}\"\n",
    "    )\n",
    "\n",
    "print(\"Constantes globales cargadas correctamente\")\n",
    "print(\"Modelo:\", MODEL_ID)\n",
    "print(\"Autor objetivo:\", target_author)\n",
    "print(\"Bot:\", bot_name)\n",
    "print(\"Directorio de salida:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932d628-6f07-4fa5-b392-29b94686ef3b",
   "metadata": {},
   "source": [
    "## Importación de librerías\n",
    "\n",
    "Cargamos todas las dependencias necesarias para el procesamiento del chat, construcción del dataset, entrenamiento (Transformers/TRL), cuantización (bitsandbytes), LoRA (PEFT), evaluación y visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c6272e-24a2-4de5-b229-f904dd785e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 20:36:30.220013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "number of GPUs: 1\n",
      "2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import math\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"done\")\n",
    "print(f\"number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83ac45-92be-43b7-b179-e20a24e47f15",
   "metadata": {},
   "source": [
    "## Configuración del dispositivo de ejecución\n",
    "\n",
    "Establecemos el dispositivo de cómputo a utilizar durante el entrenamiento y la inferencia. Utilizamos la GPU si está disponible, de lo contrario el código se ejecuta en CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8451d6b4-c471-4b56-af51-9027b8d7964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6687fe-9f74-4262-9c89-e1964f13bb63",
   "metadata": {},
   "source": [
    "## Parámetros de filtrado y mensajes irrelevantes\n",
    "\n",
    "Definimos el conjunto de patrones que identifican mensajes automáticos o irrelevantes generados por WhatsApp (por ejemplo, avisos del sistema o contenido multimedia omitido). Estos patrones se utilizan para filtrar el chat antes de construir los ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e028e9-40b1-4b28-854a-d52d27791a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevantData = {\n",
    "    # Spanish system messages\n",
    "    'eliminaste este mensaje',\n",
    "    'se eliminó este mensaje',\n",
    "    '<multimedia omitido>',\n",
    "    'multimedia omitido',\n",
    "    'los mensajes y las llamadas están cifrados de extremo a extremo',\n",
    "    # English system messages\n",
    "    'you deleted this message',\n",
    "    'this message was deleted',\n",
    "    '<media omitted>',\n",
    "    'media omitted',\n",
    "    'messages and calls are end-to-end encrypted',\n",
    "}\n",
    "\n",
    "def containsIrrelevantData(message: str) -> bool:\n",
    "    \"\"\"\n",
    "    Assumes the input message is already lowercased.\n",
    "    Returns True if the message contains any known WhatsApp system or noise pattern.\n",
    "    \"\"\"\n",
    "    msg = message.lower()\n",
    "    return any(irr in msg for irr in irrelevantData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da54b6-a85b-4d3b-a572-f813cd000f05",
   "metadata": {},
   "source": [
    "## Tokenizer del modelo\n",
    "\n",
    "Cargamos el tokenizer asociado al modelo base. Se utiliza durante el preprocesamiento para convertir el historial de conversación al formato exacto requerido por el chat template del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62c9069-0174-437a-ba7f-ea83cfba6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a48ffc-84f6-42cb-b17e-eefd94a5e5cd",
   "metadata": {},
   "source": [
    "## Procesamiento del chat de WhatsApp y formateo para el modelo\n",
    "\n",
    "Convertimos el chat exportado de WhatsApp en ejemplos de entrenamiento listos para fine-tuning. Se agrupan mensajes consecutivos por autor, se construye un historial acotado por k_history y time_gap, y finalmente se serializa cada ejemplo usando el chat_template del tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5866af4-710a-4a1b-8936-29bc96621cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Procesando chat (k-turns con roles)...\n",
      "📜 Total turnos agrupados: 47431\n",
      "✅ Se generaron 22672 ejemplos de entrenamiento.\n",
      "💾 Archivo guardado en: ./chatbot_NicoBotSmol/formatted_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light text cleaning: lowercasing, whitespace normalization, and basic character filtering.\n",
    "    Noise filtering (WhatsApp system messages) is handled by `containsIrrelevantData`.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-záéíóúñü0-9,.;:¡!¿?\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def parse_datetime(line: str):\n",
    "    \"\"\"Extracts WhatsApp timestamp if present; returns datetime or None.\"\"\"\n",
    "    match = re.match(r\"(\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+)\\s-\", line)\n",
    "    if match:\n",
    "        for fmt in (\"%d/%m/%y %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
    "            try:\n",
    "                return datetime.strptime(match.group(1).replace(\",\", \"\"), fmt)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def group_consecutive_messages(messages):\n",
    "    \"\"\"\n",
    "    Groups consecutive messages from the same author into a single turn if they are close in time.\n",
    "    Multiple messages within the same turn are joined using MSG_SEP.\n",
    "    \"\"\"\n",
    "    grouped = []\n",
    "    for author, msg, ts in messages:\n",
    "        if (\n",
    "            grouped\n",
    "            and grouped[-1][0] == author\n",
    "            and ts and grouped[-1][2]\n",
    "            and (ts - grouped[-1][2]) < timedelta(hours=1)\n",
    "        ):\n",
    "            # Same author within the time window -> merge into the previous turn\n",
    "            prev_msg = grouped[-1][1]\n",
    "            new_msg = prev_msg + f\" {MSG_SEP} \" + msg\n",
    "            grouped[-1] = (author, new_msg, ts)\n",
    "        else:\n",
    "            grouped.append((author, msg, ts))\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def process_whatsapp_chat_with_roles(\n",
    "    filepath,\n",
    "    k_history=4,\n",
    "    time_gap=timedelta(hours=3),\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds training samples from a WhatsApp export:\n",
    "      - Turns are consecutive messages from the same author (grouped)\n",
    "      - A backward context window is built using k_history and time_gap\n",
    "      - Roles follow the standard chat format: user / assistant\n",
    "      - The final text is serialized using tokenizer.apply_chat_template(...)\n",
    "    \"\"\"\n",
    "    print(\"Procesando chat (k-turns con roles)...\")\n",
    "\n",
    "    messages = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            ts = parse_datetime(line)\n",
    "            match = re.match(r'\\d+/\\d+/\\d+[, ]\\s?\\d+:\\d+\\s-\\s([^:]+):\\s(.+)', line)\n",
    "            if match:\n",
    "                author = match.group(1).strip()\n",
    "                raw_msg = match.group(2)\n",
    "                msg = clean_text(raw_msg)\n",
    "                # Skip empty messages and WhatsApp system/noise lines\n",
    "                if msg and not containsIrrelevantData(msg):\n",
    "                    messages.append((author, msg, ts))\n",
    "\n",
    "    if not messages:\n",
    "        print(\"No se encontraron mensajes válidos.\")\n",
    "        return [], []\n",
    "\n",
    "    messages = group_consecutive_messages(messages)\n",
    "    print(f\"Total turnos agrupados: {len(messages)}\")\n",
    "\n",
    "    prompts, responses = [], []\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(1, len(messages)):\n",
    "        author_i, msg_i, ts_i = messages[i]\n",
    "\n",
    "        # Only keep samples where the target author is the assistant\n",
    "        if author_i != target_author:\n",
    "            continue\n",
    "\n",
    "        conversation_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener \"\n",
    "                    f\"conversaciones casuales. Respondes con la misma personalidad que {target_author} \"\n",
    "                    f\"tiene en los chats de WhatsApp./no_think\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        temp_history = []\n",
    "        last_ts = ts_i\n",
    "\n",
    "        for j in range(i - 1, -1, -1):\n",
    "            a_j, m_j, ts_j = messages[j]\n",
    "            if ts_j and last_ts and (last_ts - ts_j) > time_gap:\n",
    "                break\n",
    "\n",
    "            # Map WhatsApp authors to standard chat roles\n",
    "            role = \"assistant\" if a_j == target_author else \"user\"\n",
    "            temp_history.insert(0, {\"role\": role, \"content\": m_j})\n",
    "\n",
    "            last_ts = ts_j if ts_j is not None else last_ts\n",
    "            if len(temp_history) >= k_history:\n",
    "                break\n",
    "\n",
    "        if not temp_history:\n",
    "            continue\n",
    "\n",
    "        conversation_history.extend(temp_history)\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": msg_i})\n",
    "\n",
    "        # Serialize messages into the exact format required by the model's chat template\n",
    "        final_text = tokenizer.apply_chat_template(\n",
    "            conversation_history,\n",
    "            tokenize=False,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "\n",
    "        formatted_data.append(final_text)\n",
    "\n",
    "    print(f\"Se generaron {len(formatted_data)} ejemplos de entrenamiento.\")\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "formatted_data = process_whatsapp_chat_with_roles(chat_file)\n",
    "df = pd.DataFrame({\"text\": formatted_data})\n",
    "df.to_json(\n",
    "    f\"{OUTPUT_DIR}/formatted_data.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Archivo guardado en: {OUTPUT_DIR}/formatted_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ab835-08c2-42af-9ea3-fc307062c2b9",
   "metadata": {},
   "source": [
    "## Preparación y partición del dataset\n",
    "\n",
    "Aplicamos un filtrado mínimo para eliminar entradas vacías o ruidosas (incluyendo URLs), y convertimos el resultado a un Dataset de Hugging Face. Finalmente, dividimos el conjunto en entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caa2ad9-122c-4366-a492-36edbcea7537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Dataset listo para TRL: 22185 conversaciones.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(f\"{OUTPUT_DIR}/formatted_data.jsonl\", lines=True)\n",
    "\n",
    "# Basic cleanup: drop empty or extremely short samples\n",
    "data = data[data[\"text\"].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "# URL filtering (original logic)\n",
    "data = data[~data[\"text\"].str.contains(r\"http|www|\\.com\", regex=True)]\n",
    "\n",
    "print(f\"Dataset listo para TRL: {len(data)} conversaciones.\")\n",
    "\n",
    "# Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Train/validation split\n",
    "dataset = dataset.train_test_split(test_size=0.1) # 10% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5d317-9de9-4c14-bf66-6b5472db12dc",
   "metadata": {},
   "source": [
    "## Carga del modelo y configuración de LoRA\n",
    "\n",
    "Configuramos la cuantización en 4 bits para reducir el consumo de memoria, cargamos el modelo base en GPU y preparamos la adaptación mediante LoRA. Este enfoque permite entrenar únicamente un subconjunto reducido de parámetros, haciendo el fine-tuning más eficiente sin modificar los pesos originales del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d60446b-ad4f-4e26-bf6a-1e642c35c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Cargando modelo HuggingFaceTB/SmolLM3-3B en 4-bits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a364a0a51c414e289d7fb751f4f6cbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧐 Memoria del modelo: torch.bfloat16\n",
      "⚖️ Footprint de memoria: 1.80 GB\n"
     ]
    }
   ],
   "source": [
    "# Quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "print(f\"⏳ Cargando modelo {MODEL_ID} en 4-bits...\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",  # disabled to control device placement explicitly\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Tokenizer was loaded previously\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Common fix for LLaMA/SmolLM-style models\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) configuration\n",
    "# Only a small subset of parameters will be trained\n",
    "peft_config = LoraConfig(\n",
    "    r=16,        # Attention rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Trainable modules\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Memoria del modelo: {model.dtype}\")\n",
    "print(f\"Footprint de memoria: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979940f-926a-46ff-96eb-6898d453a2fb",
   "metadata": {},
   "source": [
    "## Configuración del entrenador SFT\n",
    "\n",
    "Definimos los hiperparámetros de entrenamiento. Se pueden ajustan algunos como épocas, batch size efectivo y tasa de aprendizaje, priorizando estabilidad y uso eficiente de memoria. Finalmente, inicializamos el SFTTrainer, que integra el modelo, los datos y la configuración LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ced775-8ba0-41a0-b9f8-bfa9d0a6eb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea74dd3f24e24054bb071b3c52978ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/19966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29caedfef46640ca963b87f828dbcfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/19966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd17bea6fc24dc9b0587b0f7f0c3551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/19966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8296124d20164926980b52572a734d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab625ca23c274ba6bea27e545aa49471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaa4b1149d14fd5a1682e069b393387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    # Evaluation disabled to avoid OOM due to logits accumulation\n",
    "    eval_strategy=\"no\",\n",
    "\n",
    "    save_total_limit=2,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Checkpointing and logging\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673e4ed-125d-4259-b32b-3614c26f7272",
   "metadata": {},
   "source": [
    "## Entrenamiento y guardado del modelo\n",
    "\n",
    "Ejecutamos el proceso de fine-tuning utilizando SFTTrainer y, una vez finalizado, liberamos memoria de GPU para evitar fragmentación. Finalmente, guardamos el modelo entrenado y el tokenizer en el directorio de salida definido previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e20a7891-4346-4a99-b32a-0871f91866be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 128012}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento con SFTTrainer...\n",
      "El modelo será guardado en ./chatbot_NicoBotSmol\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7488' max='7488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7488/7488 2:26:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.200800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.184900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.249900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.166800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>1.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>1.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>1.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>1.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>1.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>1.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>1.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>1.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>1.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>1.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>1.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>1.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>1.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>1.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>1.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>1.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>1.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>1.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>1.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>1.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>1.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>1.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>1.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>1.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>1.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>1.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>1.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>1.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>1.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>1.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>1.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>1.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>1.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>1.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>1.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>1.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>1.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>1.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>1.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>1.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>1.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>1.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>1.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>1.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>1.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>1.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>1.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>1.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>1.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>1.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>1.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>1.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>1.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>1.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>1.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>1.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>1.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>1.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>1.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>1.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>0.998400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>1.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>1.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>1.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>1.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>1.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>1.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>1.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>1.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>1.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>1.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>1.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>1.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>1.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>1.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>1.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>1.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>1.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>1.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>1.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>1.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>1.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4630</td>\n",
       "      <td>1.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>1.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>1.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4670</td>\n",
       "      <td>1.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4690</td>\n",
       "      <td>1.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4710</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4730</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>1.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4770</td>\n",
       "      <td>1.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>1.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4810</td>\n",
       "      <td>1.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>1.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4830</td>\n",
       "      <td>1.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>1.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4870</td>\n",
       "      <td>1.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4890</td>\n",
       "      <td>1.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4910</td>\n",
       "      <td>1.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>1.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4930</td>\n",
       "      <td>0.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4970</td>\n",
       "      <td>1.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>1.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>1.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5010</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>0.973400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5030</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5070</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5090</td>\n",
       "      <td>1.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5110</td>\n",
       "      <td>0.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>1.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5130</td>\n",
       "      <td>0.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>0.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5170</td>\n",
       "      <td>1.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>1.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5190</td>\n",
       "      <td>0.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>1.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5230</td>\n",
       "      <td>1.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>1.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5270</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>0.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5290</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5310</td>\n",
       "      <td>0.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>1.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5330</td>\n",
       "      <td>0.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>0.981800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5370</td>\n",
       "      <td>0.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>1.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>0.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5410</td>\n",
       "      <td>1.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>1.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5430</td>\n",
       "      <td>1.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>0.984500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.944600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5470</td>\n",
       "      <td>1.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5490</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5510</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>1.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5530</td>\n",
       "      <td>1.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>1.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>1.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5590</td>\n",
       "      <td>0.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5610</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>1.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5630</td>\n",
       "      <td>1.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>1.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>0.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>1.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5690</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5730</td>\n",
       "      <td>0.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>0.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>0.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5770</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5790</td>\n",
       "      <td>0.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>1.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>1.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5870</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5890</td>\n",
       "      <td>1.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5910</td>\n",
       "      <td>1.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5930</td>\n",
       "      <td>0.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>1.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5970</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5990</td>\n",
       "      <td>1.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6010</td>\n",
       "      <td>1.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6030</td>\n",
       "      <td>1.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6070</td>\n",
       "      <td>1.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>0.945500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6090</td>\n",
       "      <td>0.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6110</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>1.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6130</td>\n",
       "      <td>1.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>1.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6170</td>\n",
       "      <td>1.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6190</td>\n",
       "      <td>0.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6210</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6230</td>\n",
       "      <td>1.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>1.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>0.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6270</td>\n",
       "      <td>0.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>0.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6290</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6310</td>\n",
       "      <td>0.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>1.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6330</td>\n",
       "      <td>1.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6370</td>\n",
       "      <td>0.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>1.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>1.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6410</td>\n",
       "      <td>1.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>1.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6430</td>\n",
       "      <td>1.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>1.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>0.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6470</td>\n",
       "      <td>0.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>1.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6490</td>\n",
       "      <td>1.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6510</td>\n",
       "      <td>1.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>1.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6530</td>\n",
       "      <td>0.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>1.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>1.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6570</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>1.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6590</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6610</td>\n",
       "      <td>0.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>0.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6630</td>\n",
       "      <td>0.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>1.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>0.925900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>1.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6690</td>\n",
       "      <td>1.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6710</td>\n",
       "      <td>1.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>0.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6730</td>\n",
       "      <td>0.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>0.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6760</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6770</td>\n",
       "      <td>1.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6780</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6790</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6810</td>\n",
       "      <td>1.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6830</td>\n",
       "      <td>1.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6840</td>\n",
       "      <td>1.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>0.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6870</td>\n",
       "      <td>0.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>1.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6890</td>\n",
       "      <td>1.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6910</td>\n",
       "      <td>1.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6920</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6930</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6940</td>\n",
       "      <td>1.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6960</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6970</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6980</td>\n",
       "      <td>1.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6990</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7010</td>\n",
       "      <td>0.955400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>1.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7030</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7040</td>\n",
       "      <td>1.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>1.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7070</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7080</td>\n",
       "      <td>0.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7090</td>\n",
       "      <td>0.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7110</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7120</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7130</td>\n",
       "      <td>0.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7160</td>\n",
       "      <td>1.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7170</td>\n",
       "      <td>1.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7180</td>\n",
       "      <td>1.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7190</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.997300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7210</td>\n",
       "      <td>0.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7220</td>\n",
       "      <td>0.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7230</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7240</td>\n",
       "      <td>1.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7260</td>\n",
       "      <td>1.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7270</td>\n",
       "      <td>1.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7280</td>\n",
       "      <td>1.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7290</td>\n",
       "      <td>0.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7310</td>\n",
       "      <td>1.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7320</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7330</td>\n",
       "      <td>1.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7340</td>\n",
       "      <td>0.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7370</td>\n",
       "      <td>1.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7380</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7390</td>\n",
       "      <td>1.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7410</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7420</td>\n",
       "      <td>0.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7430</td>\n",
       "      <td>0.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>1.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>0.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7480</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./chatbot_NicoBotSmol/tokenizer_config.json',\n",
       " './chatbot_NicoBotSmol/special_tokens_map.json',\n",
       " './chatbot_NicoBotSmol/chat_template.jinja',\n",
       " './chatbot_NicoBotSmol/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Iniciando entrenamiento con SFTTrainer...\\n\"\n",
    "    f\"El modelo será guardado en {OUTPUT_DIR}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "finally:\n",
    "    # Ensure GPU memory is released even if training is interrupted\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save trained model and tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b17843-d244-48de-ad0b-9312f91fac2e",
   "metadata": {},
   "source": [
    "## Compresión del modelo entrenado\n",
    "\n",
    "Empaquetamos el directorio de salida en un archivo .zip para facilitar su descarga y almacenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7895b1-d24f-4f93-84d1-9d5047780082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: chatbot_NicoBotSmol/ (stored 0%)\n",
      "  adding: chatbot_NicoBotSmol/formatted_data.jsonl (deflated 92%)\n",
      "  adding: chatbot_NicoBotSmol/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: chatbot_NicoBotSmol/.ipynb_checkpoints/formatted_data-checkpoint.jsonl (deflated 92%)\n",
      "  adding: chatbot_NicoBotSmol/README.md (deflated 44%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/ (stored 0%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/README.md (deflated 65%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/adapter_config.json (deflated 57%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/training_args.bin (deflated 53%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/optimizer.pt (deflated 24%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/scheduler.pt (deflated 61%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/rng_state.pth (deflated 26%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7450/trainer_state.json (deflated 81%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/ (stored 0%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/README.md (deflated 65%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/adapter_config.json (deflated 57%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/training_args.bin (deflated 53%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/optimizer.pt (deflated 24%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/scheduler.pt (deflated 62%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/rng_state.pth (deflated 26%)\n",
      "  adding: chatbot_NicoBotSmol/checkpoint-7488/trainer_state.json (deflated 81%)\n",
      "  adding: chatbot_NicoBotSmol/adapter_model.safetensors (deflated 22%)\n",
      "  adding: chatbot_NicoBotSmol/adapter_config.json (deflated 57%)\n",
      "  adding: chatbot_NicoBotSmol/chat_template.jinja (deflated 68%)\n",
      "  adding: chatbot_NicoBotSmol/tokenizer_config.json (deflated 96%)\n",
      "  adding: chatbot_NicoBotSmol/special_tokens_map.json (deflated 41%)\n",
      "  adding: chatbot_NicoBotSmol/tokenizer.json (deflated 85%)\n",
      "  adding: chatbot_NicoBotSmol/training_args.bin (deflated 53%)\n",
      "⚠ Luego de que la carpeta se haya comprimido, no te olvides de descargarla\n"
     ]
    }
   ],
   "source": [
    "file_zip = bot_name + \"_compressed.zip\"\n",
    "\n",
    "# Compress the output directory into a ZIP archive\n",
    "!zip -r {file_zip} {OUTPUT_DIR}\n",
    "\n",
    "print(\"⚠ Luego de que la carpeta se haya comprimido, no te olvides de descargarla\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60ea94-ec2a-4efe-82e7-aab8183824de",
   "metadata": {},
   "source": [
    "## Restauración del modelo entrenado\n",
    "\n",
    "Descomprimimos el archivo y restauramos el modelo sin necesidad de volver a ejecutar todo el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0173c2-4463-4fe5-9f97-b61f6a6e80d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  NicoBotSmol_compressed.zip\n",
      "replace chatbot_NicoBotSmol/formatted_data.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "✅ Carpeta descomprimida\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_zip = bot_name + \"_compressed.zip\"\n",
    "# To avoid rerunning the full pipeline, upload the previously generated ZIP\n",
    "# and execute the following command to restore the directory\n",
    "!unzip {file_zip}\n",
    "\n",
    "print(\"✅ Carpeta descomprimida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ece9-b6eb-4426-9da6-dbc31355264f",
   "metadata": {},
   "source": [
    "## Comparación cualitativa entre modelo base y modelo fine-tuneado\n",
    "\n",
    "Generamos respuestas con el modelo base y con el modelo fine tunned usando exactamente el mismo prompt. Esto permite evaluar de manera rápida el impacto del fine-tuning de forma cualitativa, comparando estilo, coherencia y fidelidad al comportamiento esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b33da88-bbf0-4fce-807f-29e1af176759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e453770f5ce4c628e149108af1be977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8d4fcc86294185be8d6a441202e8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACIÓN DE MODELOS ===\n",
      "\n",
      "PREGUNTA: Hola, cómo estás?\n",
      "--------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡Hola! Me siento genial, gracias por preguntar. ¿Y tú? ¿Cómo te va hoy? Espero que todo esté bien y que tengas un buen día. Si necesitas algo o quieres charlar un poco más, estoy aquí para ayudarte. ¡Hasta pronto!\n",
      "--------------------\n",
      "BOT (Fine-Tuned):\n",
      "bien y vos?? <|msg_sep|> te levantaste temprano\n",
      "==================================================\n",
      "\n",
      "PREGUNTA: Eu sale algo el finde?\n",
      "--------------------------------------------------\n",
      "Modelo BASE:\n",
      "Claro, siempre es divertido salir el fin de semana! ¿Qué tal si planeamos algo juntos? Podemos ir al cine, a un concierto o simplemente pasear por la ciudad. ¡Dime qué te gusta más y lo organizo!\n",
      "--------------------\n",
      "BOT (Fine-Tuned):\n",
      "si <|msg_sep|> después nos ponemos a ver si podemos hacerlo juntos y arreglar lo q no funcione\n",
      "==================================================\n",
      "\n",
      "PREGUNTA: Qué opinás de la programación?\n",
      "--------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡Hola! Que buena pregunta. La programación es algo realmente fascinante para mí. Me encanta cómo funciona el código y cómo las personas pueden crear soluciones a problemas complejos utilizando solo instrucciones simples. Es como una especie de magia, pero basada en lógica y matemáticas. Además, me gusta ver cómo los desarrolladores resuelven bugs o optimizan algoritmos. Siempre hay más que aprender y mejorar. ¿Tú qué\n",
      "--------------------\n",
      "BOT (Fine-Tuned):\n",
      "mm es lo único q puedo hacer jaja <|msg_sep|> no tengo ni idea de nada más xd\n",
      "==================================================\n",
      "\n",
      "PREGUNTA: Me aburrooo, contame algo\n",
      "--------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡Claro que sí! Si te aburres, aquí tienes una idea:\n",
      "\n",
      "Imagina que eres el capitán de una nave espacial en una misión para explorar un planeta desconocido. Tu tripulación está compuesta por personas de diferentes culturas y orígenes, y juntos enfrentan desafíos inesperados mientras intentan sobrevivir en este nuevo mundo. ¿Qué tipo de aventura imaginas? ¿Hay algún fenó\n",
      "--------------------\n",
      "BOT (Fine-Tuned):\n",
      "que te gustaría saber xd <|msg_sep|> yo estoy viendo si puedo aprender a tocar el piano? <|msg_sep|> pero no sé cómo empezar :c\n",
      "==================================================\n",
      "\n",
      "PREGUNTA: Nos vemos más tarde?\n",
      "--------------------------------------------------\n",
      "Modelo BASE:\n",
      "Claro, nos vemos más tarde! ¿Qué tal si preparamos algo especial para esa reunión? Siéntete libre de sugerir cualquier cosa, ¡estoy aquí para ayudarte a planearlo!\n",
      "--------------------\n",
      "BOT (Fine-Tuned):\n",
      "si a las 17 <|msg_sep|> si no hay problema y no podes venís otro día avísame\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model (no fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "fine_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    use_cache=True\n",
    ")\n",
    "fine_base_model.to(device)\n",
    "\n",
    "# Load fine-tuned model (inject LoRA adapters into the base model)\n",
    "fine_model = PeftModel.from_pretrained(fine_base_model, OUTPUT_DIR).to(device)\n",
    "\n",
    "def generate_response(prompt_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a response using the model's chat template.\n",
    "    \"\"\"\n",
    "    # 1) Chat message format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                f\"Eres {bot_name}, un bot de {target_author}, que se encarga de mantener \"\n",
    "                f\"conversaciones casuales. Respondes con la misma personalidad que {target_author} \"\n",
    "                f\"tiene en los chats de WhatsApp./no_think\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_text},\n",
    "    ]\n",
    "\n",
    "    # 2) Apply chat template and prepare tensors\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 3) Generate\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # 4) Decode and post-process\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove any <think>...</think> blocks if present\n",
    "    decoded = re.sub(r\"<think>.*?</think>\", \"\", decoded, flags=re.DOTALL)\n",
    "\n",
    "    # Heuristic: keep only the portion after the user's prompt\n",
    "    if prompt_text in decoded:\n",
    "        response = decoded.split(prompt_text)[-1].strip()\n",
    "    else:\n",
    "        response = decoded\n",
    "\n",
    "    # Extra cleanup in case role markers leak into the final text\n",
    "    for tag in [\"system\", \"user\", \"assistant\"]:\n",
    "        response = response.replace(tag, \"\")\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def compare_models(prompt):\n",
    "    print(f\"\\nPREGUNTA: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Base model\n",
    "    try:\n",
    "        base_resp = generate_response(prompt, base_model, tokenizer)\n",
    "        print(f\"Modelo BASE:\\n{base_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Base: {e}\")\n",
    "\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Fine-tuned model\n",
    "    try:\n",
    "        fine_resp = generate_response(prompt, fine_model, tokenizer)\n",
    "        print(f\"BOT (Fine-Tuned):\\n{fine_resp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error del Modelo Fine-Tuned: {e}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🧪 PRUEBA MANUAL\n",
    "# ============================================================\n",
    "mis_preguntas = [\n",
    "    \"Hola, cómo estás?\",\n",
    "    \"Eu sale algo el finde?\",\n",
    "    \"Qué opinás de la programación?\",\n",
    "    \"Me aburrooo, contame algo\",\n",
    "    \"Nos vemos más tarde?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== COMPARACIÓN DE MODELOS ===\")\n",
    "for p in mis_preguntas:\n",
    "    compare_models(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aeb5c-29f1-47d0-bab6-c37df125f2a8",
   "metadata": {},
   "source": [
    "## Comparación automática usando ejemplos del conjunto de validación\n",
    "\n",
    "Tomamos una muestra aleatoria del conjunto de validación y comparamos las respuestas del modelo base vs el modelo fine-tuneado, reutilizando la misma función de generación. Esto permite inspeccionar rápidamente diferencias de estilo y coherencia sobre datos reales del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e35b45da-d49c-47a6-aa82-9a3787c13029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARACIÓN AUTOMÁTICA (CONJUNTO DE VALIDACIÓN) ===\n",
      "Se evaluarán 5 ejemplos aleatorios del split 'test'.\n",
      "\n",
      "================================================================================\n",
      "Ejemplo #571\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt (último mensaje del usuario):\n",
      "si bueno pero una amiga tiene un baño re lindo para sacarse fotos jaja\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡Qué genial! Si ella es tan bonita como su baño, imagina lo hermosa que debe ser. Me pregunto si el espejo también está decorado con imágenes preciosas o tal vez hay toallas y accesorios muy elegantes. ¿Tiene algún lugar especial donde se pinte las uñas? ¡Me encantaría ver esas fotos por aquí!\n",
      "\n",
      "¿Y tú, tienes algún lugar favorito para tomar fotos? A veces me gusta usar\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo FINE-TUNEADO:\n",
      "jajaja <|msg_sep|> ah y te lo regalan?\n",
      "\n",
      "================================================================================\n",
      "Ejemplo #419\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt (último mensaje del usuario):\n",
      "si xd pense q habia un par de horas de diferencia nomas\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡Claro! A veces las diferencias horarias pueden confundirnos un poco, pero aquí estamos para aclararlas y seguir disfrutando de nuestras conversaciones. ¿Qué tal si nos tomamos un descanso? Tal vez podamos volver a charlar más tarde cuando estemos más alineados en el tiempo. Hasta entonces, ¡espero que tengas una buena jornada!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo FINE-TUNEADO:\n",
      "que lindo <|msg_sep|> y tenés tiempo para descansar?\n",
      "\n",
      "================================================================================\n",
      "Ejemplo #356\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt (último mensaje del usuario):\n",
      "ademas de la tablet y el cuaderno? y la ropa que cuenta como regalo de parte de mi nona\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo BASE:\n",
      "Claro, vamos a pensar juntos qué más podrías considerar para tu lista de regalos:\n",
      "\n",
      "1. **Libros**: ¿Hay algún libro o serie que te guste mucho y no puedas leer por ti mismo?\n",
      "2. **Accesorios de moda**: Zapatos, bolsos, joyería.\n",
      "3. **Disfraces temáticos**: Para Halloween o eventos especiales.\n",
      "4. **Productos de belleza**: Crema facial, mascarillas,\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo FINE-TUNEADO:\n",
      "si <|msg_sep|> yo me había hecho una lista pero no llegue a hacer nada todavía jaja <|msg_sep|> te voy a buscar algo para tus cumpleños cuando sea tu cumple\n",
      "\n",
      "================================================================================\n",
      "Ejemplo #1728\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt (último mensaje del usuario):\n",
      "ufff x tengo mucho miedo de hacer ejercicios x mi cuenta xd\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo BASE:\n",
      "¡No te preocupes tanto! Es completamente normal sentirse nervioso al empezar algo nuevo y especialmente cuando hablamos de ejercicio. Lo importante es dar pequeños pasos y no presionarte demasiado. Puedes comenzar poco a poco, como caminar durante unos minutos cada día, o incluso practicar estiramientos suaves mientras ves tu serie favorita. Recuerda que lo más importante es disfrutar del proceso y celebrar cada pequeño logro.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo FINE-TUNEADO:\n",
      "que pasa? <|msg_sep|> te habrá quedado poco tiempo para rendir? <|msg_sep|> o no vas a poder promocionar si no lo haces?\n",
      "\n",
      "================================================================================\n",
      "Ejemplo #130\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt (último mensaje del usuario):\n",
      "galletitas con mate cocido\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo BASE:\n",
      "son una delicia típica argentina y muy popular en muchas regiones de Sudamérica. Aquí te doy una receta sencilla para prepararlas:\n",
      "\n",
      "### Ingredientes:\n",
      "- 2 tazas de harina\n",
      "- 1/2 cucharadita de polvo para hornear\n",
      "- 1/4 cucharadita de sal\n",
      "- 1/2 taza de az\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Modelo FINE-TUNEADO:\n",
      "nice <|msg_sep|> te quedaste hasta tarde?\n",
      "\n",
      "================================================================================\n",
      "Comparación finalizada. Ejemplos mostrados: 5/5.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# --- Safety checks\n",
    "if \"dataset\" not in globals():\n",
    "    raise RuntimeError(\"La variable 'dataset' no existe. Ejecutá primero la preparación del dataset.\")\n",
    "\n",
    "if \"base_model\" not in globals() or \"fine_model\" not in globals():\n",
    "    raise RuntimeError(\"base_model y/o fine_model no están cargados. Ejecutá primero el comparador de modelos.\")\n",
    "\n",
    "if \"tokenizer\" not in globals():\n",
    "    raise RuntimeError(\"La variable 'tokenizer' no existe. Asegurate de cargar el tokenizer primero.\")\n",
    "\n",
    "if \"generate_response\" not in globals():\n",
    "    raise RuntimeError(\"La función 'generate_response' no existe. Ejecutá primero la celda del comparador.\")\n",
    "\n",
    "# --- Robust extraction for your serialized format (<|im_start|>user ... <|im_end|>)\n",
    "IM_USER_BLOCK_RE = re.compile(\n",
    "    r\"<\\|im_start\\|>\\s*user\\s*(.*?)\\s*<\\|im_end\\|>\",\n",
    "    flags=re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_last_user_message(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the last user message from a chat-template serialized sample.\n",
    "    This implementation matches the format shown in your dataset:\n",
    "      <|im_start|>user ... <|im_end|>\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    matches = IM_USER_BLOCK_RE.findall(text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "\n",
    "    user_msg = matches[-1].strip()\n",
    "\n",
    "    # Minimal cleanup for display / prompting\n",
    "    user_msg = user_msg.replace(\"<|msg_sep|>\", \" \")\n",
    "    user_msg = re.sub(r\"\\s+\", \" \", user_msg).strip()\n",
    "    return user_msg\n",
    "\n",
    "# --- Validation sampling\n",
    "val_split = dataset[\"test\"]\n",
    "n_samples = min(5, len(val_split))\n",
    "\n",
    "if n_samples == 0:\n",
    "    raise RuntimeError(\"El split de validación está vacío (dataset['test']). Revisá el train_test_split.\")\n",
    "\n",
    "sample_idx = random.sample(range(len(val_split)), n_samples)\n",
    "\n",
    "print(\"\\n=== COMPARACIÓN AUTOMÁTICA (CONJUNTO DE VALIDACIÓN) ===\")\n",
    "print(f\"Se evaluarán {n_samples} ejemplos aleatorios del split 'test'.\")\n",
    "\n",
    "# --- Compare base vs fine-tuned on validation prompts\n",
    "shown = 0\n",
    "for idx in sample_idx:\n",
    "    sample_text = val_split[idx][\"text\"]\n",
    "    user_prompt = extract_last_user_message(sample_text)\n",
    "\n",
    "    if not user_prompt:\n",
    "        continue\n",
    "\n",
    "    shown += 1\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Ejemplo #{idx}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Prompt (último mensaje del usuario):\")\n",
    "    print(user_prompt)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Modelo BASE:\")\n",
    "    try:\n",
    "        print(generate_response(user_prompt, base_model, tokenizer))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Modelo FINE-TUNEADO:\")\n",
    "    try:\n",
    "        print(generate_response(user_prompt, fine_model, tokenizer))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Comparación finalizada. Ejemplos mostrados: {shown}/{n_samples}.\")\n",
    "if shown < n_samples:\n",
    "    print(\"Nota: algunos ejemplos no pudieron usarse porque no se detectó el bloque <|im_start|>user ... <|im_end|>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cec77-3e79-46b4-8492-e0533fca9c7e",
   "metadata": {},
   "source": [
    "## Análisis estadístico y calidad del dataset\n",
    "\n",
    "Este bloque analiza el dataset conversacional antes y después del filtrado mínimo aplicado durante el preprocesamiento.\n",
    "El objetivo es comprender la estructura de los datos, evaluar su calidad y verificar que el conjunto final sea adecuado para el proceso de fine-tuning supervisado.\n",
    "\n",
    "En particular, se reportan las siguientes métricas:\n",
    "\n",
    "* *Dataset*: Identifica si las estadísticas corresponden al conjunto original o al conjunto filtrado.\n",
    "\n",
    "* *Samples*: Cantidad total de ejemplos de entrenamiento. Cada ejemplo corresponde a una conversación serializada que el modelo debe aprender a continuar.\n",
    "\n",
    "* *Exact duplicates*: Número de ejemplos idénticos detectados. Un valor bajo indica que no hay sobre-representación artificial de conversaciones repetidas.\n",
    "\n",
    "* *Chars avg*: Longitud promedio de los ejemplos medida en caracteres. Da una noción general del tamaño del texto sin depender del tokenizer.\n",
    "\n",
    "* *Words avg*: Cantidad promedio de palabras por ejemplo, utilizada como medida intuitiva de longitud del contenido.\n",
    "\n",
    "* *Words p95*: Percentil 95 de la longitud en palabras. Indica que el 95% de los ejemplos tiene una longitud menor o igual a este valor, y permite identificar conversaciones largas.\n",
    "\n",
    "* *Unique words*: Cantidad total de palabras distintas presentes en el dataset (tamaño del vocabulario).\n",
    "\n",
    "* *Vocab richness (%)*: Proporción entre palabras únicas y el total de palabras. Valores bajos son esperables en chats personales y reflejan consistencia de estilo y repetición de expresiones.\n",
    "\n",
    "* *Tokens avg*: Cantidad promedio de tokens por ejemplo según el tokenizer del modelo, que representa lo que efectivamente procesa el modelo.\n",
    "\n",
    "* *Tokens p95*: Percentil 95 de la longitud en tokens. Es una métrica clave para estimar el uso de contexto y evitar truncamiento durante el entrenamiento.\n",
    "\n",
    "Además, se incluye un ranking de las palabras más frecuentes, útil para:\n",
    "\n",
    "* Detectar muletillas o expresiones dominantes\n",
    "\n",
    "* Identificar ruido residual\n",
    "\n",
    "* Caracterizar el estilo conversacional aprendido por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fc1562b-c343-4518-bccc-843c8eee4ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>samples</th>\n",
       "      <th>exact_duplicates</th>\n",
       "      <th>chars_avg</th>\n",
       "      <th>words_avg</th>\n",
       "      <th>words_p95</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>vocab_richness_pct</th>\n",
       "      <th>tokens_avg</th>\n",
       "      <th>tokens_p95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original (sin filtrar)</td>\n",
       "      <td>22672</td>\n",
       "      <td>5</td>\n",
       "      <td>865.380690</td>\n",
       "      <td>48.819601</td>\n",
       "      <td>108.0</td>\n",
       "      <td>23419</td>\n",
       "      <td>2.115847</td>\n",
       "      <td>227.214758</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filtrado</td>\n",
       "      <td>22185</td>\n",
       "      <td>5</td>\n",
       "      <td>862.769619</td>\n",
       "      <td>48.489385</td>\n",
       "      <td>107.0</td>\n",
       "      <td>22621</td>\n",
       "      <td>2.102837</td>\n",
       "      <td>226.308812</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  dataset  samples  exact_duplicates   chars_avg  words_avg  \\\n",
       "0  Original (sin filtrar)    22672                 5  865.380690  48.819601   \n",
       "1                Filtrado    22185                 5  862.769619  48.489385   \n",
       "\n",
       "   words_p95  unique_words  vocab_richness_pct  tokens_avg  tokens_p95  \n",
       "0      108.0         23419            2.115847  227.214758       353.0  \n",
       "1      107.0         22621            2.102837  226.308812       351.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔝 Palabras más frecuentes (dataset filtrado)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>42276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>que</td>\n",
       "      <td>31067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de</td>\n",
       "      <td>27559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>si</td>\n",
       "      <td>24366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me</td>\n",
       "      <td>23880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>el</td>\n",
       "      <td>18938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>la</td>\n",
       "      <td>18850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pero</td>\n",
       "      <td>18404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xd</td>\n",
       "      <td>16091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lo</td>\n",
       "      <td>14105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>13755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>se</td>\n",
       "      <td>13405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>13232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>te</td>\n",
       "      <td>10438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yo</td>\n",
       "      <td>9173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>un</td>\n",
       "      <td>9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>una</td>\n",
       "      <td>8210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>con</td>\n",
       "      <td>7952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ya</td>\n",
       "      <td>7779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bien</td>\n",
       "      <td>7695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency\n",
       "0     no      42276\n",
       "1    que      31067\n",
       "2     de      27559\n",
       "3     si      24366\n",
       "4     me      23880\n",
       "5     el      18938\n",
       "6     la      18850\n",
       "7   pero      18404\n",
       "8     xd      16091\n",
       "9     lo      14105\n",
       "10    es      13755\n",
       "11    se      13405\n",
       "12    en      13232\n",
       "13    te      10438\n",
       "14    yo       9173\n",
       "15    un       9001\n",
       "16   una       8210\n",
       "17   con       7952\n",
       "18    ya       7779\n",
       "19  bien       7695"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Información para el informe\n",
      "• Tipo de fine-tuning: SFT (Supervised Fine-Tuning) con LoRA/QLoRA usando TRL SFTTrainer\n",
      "• Tamaño del dataset original: 22672 ejemplos\n",
      "• Tamaño del dataset filtrado: 22185 ejemplos\n",
      "• Reducción por filtrado: 2.15%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "assert \"df\" in globals(), \"❌ df (dataset original) no existe\"\n",
    "assert \"data\" in globals(), \"❌ data (dataset filtrado) no existe\"\n",
    "\n",
    "# --- Text statistics helpers (safe for chat templates)\n",
    "TAG_RE = re.compile(r\"<\\|.*?\\|>\")\n",
    "WORD_RE = re.compile(r\"[a-záéíóúñü0-9']+\")\n",
    "\n",
    "# --- Extra cleaning for linguistic analysis\n",
    "SYSTEM_BLOCK_RE = re.compile(r\"<\\|im_start\\|>system.*?<\\|im_start\\|>\", re.DOTALL)\n",
    "THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL)\n",
    "IM_TAG_RE = re.compile(r\"<\\|im_(start|end)\\|>\")\n",
    "MSG_SEP_RE = re.compile(r\"<\\|msg_sep\\|>\")\n",
    "METADATA_RE = re.compile(r\"## metadata.*?(?=<\\|im_start\\|>|$)\", re.DOTALL)\n",
    "\n",
    "# --- Remove common template leftovers that may appear as plain words\n",
    "STOP_WORDS_EXTRA = {\n",
    "    \"user\", \"assistant\", \"system\",\n",
    "    \"metadata\", \"knowledge\", \"cutoff\", \"date\", \"today\",\n",
    "    \"reasoning\", \"mode\", \"custom\", \"instructions\",\n",
    "    \"im_start\", \"im_end\", \"think\", \"msg_sep\"\n",
    "}\n",
    "\n",
    "def normalize_for_stats(text: str) -> str:\n",
    "    # Remove system blocks and metadata\n",
    "    text = SYSTEM_BLOCK_RE.sub(\" \", text)\n",
    "    text = METADATA_RE.sub(\" \", text)\n",
    "\n",
    "    # Remove internal tags\n",
    "    text = THINK_RE.sub(\" \", text)\n",
    "    text = IM_TAG_RE.sub(\" \", text)\n",
    "    text = MSG_SEP_RE.sub(\" \", text)\n",
    "\n",
    "    # Normalize\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)  # remove raw numbers (dates, years)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_words(text: str):\n",
    "    words = WORD_RE.findall(normalize_for_stats(text))\n",
    "    # Remove template leftovers and very short junk tokens (e.g., \"q\", \"y\")\n",
    "    words = [w for w in words if w not in STOP_WORDS_EXTRA and len(w) >= 2]\n",
    "    return words\n",
    "\n",
    "def compute_stats(df_in: pd.DataFrame, name: str):\n",
    "    texts = df_in[\"text\"].astype(str).tolist()\n",
    "    n = len(texts)\n",
    "\n",
    "    char_lengths = [len(t) for t in texts]\n",
    "    word_lengths = [len(extract_words(t)) for t in texts]\n",
    "\n",
    "    vocab = Counter()\n",
    "    for t in texts:\n",
    "        vocab.update(extract_words(t))\n",
    "\n",
    "    stats = {\n",
    "        \"dataset\": name,\n",
    "        \"samples\": n,\n",
    "        \"exact_duplicates\": int(df_in[\"text\"].duplicated().sum()),\n",
    "        \"chars_avg\": sum(char_lengths) / max(1, n),\n",
    "        \"words_avg\": sum(word_lengths) / max(1, n),\n",
    "        \"words_p95\": float(pd.Series(word_lengths).quantile(0.95)) if n else 0.0,\n",
    "        \"unique_words\": int(len(vocab)),\n",
    "        \"vocab_richness_pct\": 100.0 * len(vocab) / max(1, sum(vocab.values())),\n",
    "    }\n",
    "\n",
    "    # Token statistics (if tokenizer is available)\n",
    "    if \"tokenizer\" in globals() and tokenizer is not None:\n",
    "        try:\n",
    "            token_lengths = [\n",
    "                len(tokenizer(t, add_special_tokens=False).input_ids)\n",
    "                for t in texts\n",
    "            ]\n",
    "            stats.update({\n",
    "                \"tokens_avg\": sum(token_lengths) / max(1, n),\n",
    "                \"tokens_p95\": float(pd.Series(token_lengths).quantile(0.95)) if n else 0.0,\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return stats, vocab\n",
    "\n",
    "# --- Compute statistics\n",
    "stats_original, vocab_original = compute_stats(df, \"Original (sin filtrar)\")\n",
    "stats_filtered, vocab_filtered = compute_stats(data, \"Filtrado\")\n",
    "\n",
    "summary_df = pd.DataFrame([stats_original, stats_filtered])\n",
    "\n",
    "# Columns to display\n",
    "cols = [\n",
    "    \"dataset\", \"samples\", \"exact_duplicates\",\n",
    "    \"chars_avg\", \"words_avg\", \"words_p95\",\n",
    "    \"unique_words\", \"vocab_richness_pct\"\n",
    "]\n",
    "token_cols = [c for c in [\"tokens_avg\", \"tokens_p95\"] if c in summary_df.columns]\n",
    "\n",
    "display(summary_df[cols + token_cols])\n",
    "\n",
    "# --- Top frequent words (filtered dataset)\n",
    "TOP_K = 20\n",
    "top_words = pd.DataFrame(\n",
    "    vocab_filtered.most_common(TOP_K),\n",
    "    columns=[\"word\", \"frequency\"]\n",
    ")\n",
    "\n",
    "print(\"\\n🔝 Palabras más frecuentes (dataset filtrado)\")\n",
    "display(top_words)\n",
    "\n",
    "# --- Information for the report\n",
    "FINE_TUNING_TYPE = \"SFT (Supervised Fine-Tuning) con LoRA/QLoRA usando TRL SFTTrainer\"\n",
    "\n",
    "print(\"\\n🧾 Información para el informe\")\n",
    "print(f\"• Tipo de fine-tuning: {FINE_TUNING_TYPE}\")\n",
    "print(f\"• Tamaño del dataset original: {len(df)} ejemplos\")\n",
    "print(f\"• Tamaño del dataset filtrado: {len(data)} ejemplos\")\n",
    "print(f\"• Reducción por filtrado: {100*(1 - len(data)/max(1,len(df))):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30021a23-aaf3-452d-9903-6e48a9cd6bf5",
   "metadata": {},
   "source": [
    "## Evaluación cuantitativa y reporte final\n",
    "\n",
    "Calculamos métricas cuantitativas sobre el conjunto de validación para comparar el modelo base y el modelo fine-tuneado. Se reportan la cross-entropy promedio y la perplexity aproximada, junto con una visualización comparativa que resume el impacto del fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b14e9b-b085-4e53-be7c-3641d23d432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo base...\n",
      "Midiendo métricas en 100 ejemplos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo fine-tuneado...\n",
      "Midiendo métricas en 100 ejemplos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 21.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTADOS DE EVALUACIÓN\n",
      "----------------------------------------\n",
      "Base model   → Cross-Entropy: 3.430 | Perplexity: 30.89\n",
      "Fine-tuned   → Cross-Entropy: 1.116 | Perplexity: 3.05\n",
      "Mejora relativa en Perplexity: 90.12%\n",
      "Nota: una menor perplexity indica una mejor modelización del estilo conversacional.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1757/2218530845.py:87: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  barplot = sns.barplot(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_model(model, data_iterable, tokenizer, max_batches=100):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss and perplexity for a causal language model.\n",
    "    Expects an iterable with a 'text' field containing pre-formatted chat samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    print(f\"Midiendo métricas en {max_batches} ejemplos...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(tqdm(data_iterable, total=max_batches)):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "\n",
    "            # Extract pre-formatted chat text\n",
    "            text = example[\"text\"]\n",
    "\n",
    "            # Tokenize on the fly\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=1024\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            # Causal LM loss (labels = input_ids)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            losses.append(outputs.loss.item())\n",
    "\n",
    "    if not losses:\n",
    "        return {\"cross_entropy\": 0.0, \"perplexity\": 0.0}\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    try:\n",
    "        perplexity = math.exp(avg_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    return {\"cross_entropy\": avg_loss, \"perplexity\": perplexity}\n",
    "\n",
    "\n",
    "# --- VALIDATION DATA SELECTION ---\n",
    "# Prefer an explicit validation split if available\n",
    "if isinstance(dataset, dict) and \"test\" in dataset:\n",
    "    eval_data = dataset[\"test\"]\n",
    "else:\n",
    "    # Fallback: evaluate on a small subset of the dataset\n",
    "    eval_data = dataset.select(range(min(50, len(dataset))))\n",
    "\n",
    "# --- EVALUATION RUN ---\n",
    "\n",
    "print(\"\\nEvaluando modelo base...\")\n",
    "metrics_base = evaluate_model(base_model, eval_data, tokenizer)\n",
    "\n",
    "print(\"\\nEvaluando modelo fine-tuneado...\")\n",
    "metrics_fine = evaluate_model(fine_model, eval_data, tokenizer)\n",
    "\n",
    "\n",
    "# --- REPORTING ---\n",
    "print(\"\\nRESULTADOS DE EVALUACIÓN\")\n",
    "print(\"-\" * 40)\n",
    "print(\n",
    "    f\"Base model   → Cross-Entropy: {metrics_base['cross_entropy']:.3f} | \"\n",
    "    f\"Perplexity: {metrics_base['perplexity']:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Fine-tuned   → Cross-Entropy: {metrics_fine['cross_entropy']:.3f} | \"\n",
    "    f\"Perplexity: {metrics_fine['perplexity']:.2f}\"\n",
    ")\n",
    "\n",
    "# Relative improvement\n",
    "if metrics_base[\"perplexity\"] > 0:\n",
    "    improvement = (\n",
    "        (metrics_base[\"perplexity\"] - metrics_fine[\"perplexity\"])\n",
    "        / metrics_base[\"perplexity\"]\n",
    "        * 100\n",
    "    )\n",
    "else:\n",
    "    improvement = 0.0\n",
    "\n",
    "print(f\"Mejora relativa en Perplexity: {improvement:.2f}%\")\n",
    "print(\"Nota: una menor perplexity indica una mejor modelización del estilo conversacional.\")\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "barplot = sns.barplot(\n",
    "    x=[\"Base (SmolLM)\", \"Fine-tuned\"],\n",
    "    y=[metrics_base[\"perplexity\"], metrics_fine[\"perplexity\"]],\n",
    "    palette=[\"#95a5a6\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "# Annotate bar values\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(\n",
    "        f\"{p.get_height():.1f}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 9),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Comparación de Perplexity\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092595a0-57e0-4dbc-9ca4-28967de39ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
